{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e098ec",
   "metadata": {},
   "source": [
    "### This is the code to grid search some parameters to create an RNN model\n",
    "#### References:\n",
    "- Peixeiro, M. (2022). Time series forecasting in Python. Manning. Includes the codes fromi its GitHub repo (https://github.com/marcopeix/AppliedTimeSeriesForecastingInPython).  \n",
    "Contribution: The technique for converting the series into sequenced samples and the idea for scale and reeschale data after predictions.\n",
    "- Discolll, N. (2024, January 12). Harnessing RNNs for Financial Time Series Analysis: A Python Approach. Medium. https://medium.com/@redeaddiscolll/harnessing-rnns-for-financial-time-series-analysis-a-python-approach-0669b3a25c7a.  \n",
    "Contribution: EarlyStopping function for the RNN model.\n",
    "\n",
    "#### Libraries\n",
    "- Package Pandas (2.2). (2024). [Python]. https://pandas.pydata.org/\n",
    "- Package NumPy (1.23). (2023). [Pyhton]. https://numpy.org/ - Harris, C. R., Millman, K. J., Van Der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., Van Kerkwijk, M. H., Brett, M., Haldane, A., Del Río, J. F., Wiebe, M., Peterson, P., … Oliphant, T. E. (2020). Array programming with NumPy. Nature, 585(7825), 357–362. https://doi.org/10.1038/s41586-020-2649-2\n",
    "- Droettboom, J. D. H., Michael. (2024). Package matplotlib (3.8.4) [Python]. https://matplotlib.org\n",
    "- Package scikit-learn (1.4). (2024). [Pyhton]. https://scikit-learn.org/stable/index.html\n",
    "- Package Tensorflow (2.16). (2024). [Python]. https://github.com/tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27e86681",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27e86681",
    "outputId": "1af8a1db-879d-44d0-b1df-07a82eaa1fb8"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import random as python_random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (mean_absolute_error,\n",
    "                             mean_absolute_percentage_error,\n",
    "                             mean_squared_error)\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, SimpleRNN\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "import useful_functions as uf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cs469n2XOr_-",
   "metadata": {
    "id": "cs469n2XOr_-"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "#file_path = '../data/data_orig_parameters.csv'\n",
    "#file_path = '../data/data_cleaned_RF.csv'\n",
    "file_path = '../data/data_cleaned_LASSO.csv'\n",
    "#file_path = '../data/data_cleaned_RFE.csv'\n",
    "\n",
    "# parse the date column and set it as the index of the dataframe\n",
    "df_raw = pd.read_csv(file_path, parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "# define the target variable as the first column\n",
    "target_variable = df_raw.columns[0]\n",
    "\n",
    "# Convert all columns to float\n",
    "df_raw = df_raw.astype('float64')\n",
    "\n",
    "# Define threshold to remove outliers\n",
    "remove_outliers_threshold = np.nan\n",
    "#remove_outliers_threshold = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e06252c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effectively remove outliers\n",
    "if not pd.isna(remove_outliers_threshold):\n",
    "    df_cleaned = uf.remove_outliers(df_raw.copy(), threshold=remove_outliers_threshold) # in case the threshold is defined\n",
    "else:\n",
    "    df_cleaned = df_raw.copy() # in case the threshold is not defined (NaN)\n",
    "\n",
    "# After removing outliers, fill missing values\n",
    "df_adjusted = uf.fill_missing_values(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5ef4449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test and validation set sizes\n",
    "val_size = 48 # 48 months or 4 years\n",
    "test_size = 48 # 48 months or 4 years\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_raw_total = df_adjusted.copy()[:-test_size]\n",
    "train_raw = train_raw_total[:-val_size]\n",
    "val_raw = train_raw_total[-val_size:]\n",
    "test_raw = df_adjusted.copy()[-test_size:] # Last 48 months\n",
    "\n",
    "# Fill missing values\n",
    "df_train = uf.fill_missing_values(train_raw)\n",
    "df_val = uf.fill_missing_values(val_raw)\n",
    "df_test = uf.fill_missing_values(test_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db93b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scale the dfs\n",
    "# Define the scaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_train = scaler.fit_transform(df_train) # Fit and transform the train set\n",
    "scaled_val = scaler.transform(df_val) # Transform the validation set\n",
    "scaled_test = scaler.transform(df_test) # Transform the test set\n",
    "\n",
    "# include df columns names in the train and test sets\n",
    "train = pd.DataFrame(scaled_train, columns=df_train.columns)\n",
    "val = pd.DataFrame(scaled_val, columns=df_val.columns)\n",
    "test = pd.DataFrame(scaled_test, columns=df_test.columns)\n",
    "\n",
    "# Include the index in the train and test sets\n",
    "train.index = df_train.index\n",
    "val.index = df_val.index\n",
    "test.index = df_test.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f39293",
   "metadata": {},
   "source": [
    "### Reshape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e25733e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the series to samples\n",
    "# We will use the past 12 months to predict the month in an horizon of 12 months\n",
    "\n",
    "# Define the function to create the samples\n",
    "def createXY(dataset, n_past, n_future):\n",
    "    dataX, dataY = [], [] # Initialize the lists\n",
    "    # Loop for the entire dataset\n",
    "    for i in range(n_past, len(dataset) - n_future + 1):\n",
    "        dataX.append(dataset.iloc[i - n_past:i].values)  # Past n months\n",
    "        dataY.append(dataset.iloc[i + n_future - 1, 0])  #\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "n_past = 12  # Number of past months to use\n",
    "n_future = 12  # future horizon\n",
    "\n",
    "# Create the samples for the train, validation and test sets\n",
    "X_train, Y_train = createXY(train, n_past, n_future)\n",
    "X_val, Y_val = createXY(val, n_past, n_future)\n",
    "X_test, Y_test = createXY(test, n_past, n_future)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3f8c6",
   "metadata": {},
   "source": [
    "### Let's grid to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3f7f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let´s define the seed for reproducibility\n",
    "\n",
    "def func_set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    python_random.seed(seed)\n",
    "    set_seed(seed) #tensorflow.random.set_seed(seed)\n",
    "\n",
    "# Call the function to set the seed\n",
    "func_set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15a5db0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  4 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  5 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  6 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  7 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  8 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  9 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  10 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  11 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  12 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  13 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  14 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  15 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  16 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  17 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  18 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  19 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  20 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  21 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  22 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  23 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  24 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  25 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  26 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  27 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  28 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  29 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  30 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  31 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  32 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  33 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  34 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  35 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  36 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  37 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  38 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  39 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  40 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  41 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  42 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  43 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  44 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  45 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  46 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  47 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  48 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  49 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  50 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  51 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  52 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  53 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  54 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  55 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  56 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  57 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  58 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  59 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  60 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  61 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  62 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  63 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  64 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  65 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  66 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  67 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  68 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  69 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  70 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  71 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  72 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  73 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  74 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  75 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  76 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  77 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  78 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  79 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  80 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  81 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  82 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  83 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  84 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  85 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  86 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  87 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  88 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  89 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  90 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  91 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  92 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  93 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  94 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  95 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  96 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  97 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  98 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  99 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  100 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  101 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  102 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  103 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  104 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  105 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  106 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  107 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  108 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  109 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  110 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  111 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  112 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  113 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  114 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  115 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  116 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  117 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  118 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  119 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  120 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  121 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  122 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  123 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  124 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  125 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  126 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  127 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  128 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  129 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  130 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  131 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  132 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  133 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  134 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  135 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  136 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  137 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  138 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  139 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  140 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  141 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  142 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  143 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  144 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  145 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  146 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  147 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  148 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  149 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  150 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  151 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  152 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  153 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  154 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  155 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  156 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  157 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  158 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  159 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  160 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  161 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  162 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  163 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  164 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  165 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  166 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  167 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  168 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  169 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  170 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  171 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  172 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  173 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  174 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  175 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  176 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  177 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  178 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  179 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  180 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  181 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  182 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  183 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  184 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  185 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  186 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  187 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  188 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  189 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  190 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  191 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  192 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  193 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  194 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  195 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  196 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  197 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  198 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  199 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  200 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  201 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  202 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  203 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  204 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  205 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  206 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  207 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  208 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  209 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  210 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  211 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  212 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  213 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  214 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  215 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  216 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  217 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  218 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  219 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  220 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  221 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  222 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  223 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  224 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  225 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  226 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  227 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  228 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  229 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  230 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  231 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  232 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  233 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  234 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  235 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  236 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  237 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  238 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  239 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  240 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  241 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  242 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  243 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  244 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  245 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  246 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  247 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  248 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  249 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  250 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  251 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  252 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  253 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  254 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  255 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  256 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  257 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  258 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  259 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  260 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  261 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  262 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  263 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  264 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  265 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  266 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  267 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  268 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  269 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  270 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  271 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  272 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  273 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  274 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  275 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  276 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  277 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  278 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  279 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  280 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  281 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  282 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  283 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  284 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  285 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  286 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  287 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  288 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  289 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  290 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  291 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  292 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  293 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  294 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  295 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  296 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  297 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  298 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  299 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  300 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  301 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  302 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  303 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  304 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  305 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  306 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  307 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  308 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  309 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  310 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  311 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  312 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  313 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  314 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  315 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  316 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  317 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  318 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  319 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  320 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  321 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  322 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  323 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  324 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  325 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  326 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  327 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  328 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  329 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  330 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  331 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  332 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  333 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  334 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  335 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  336 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  337 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  338 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  339 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  340 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  341 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  342 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  343 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  344 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  345 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  346 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  347 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  348 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  349 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  350 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  351 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  352 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  353 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  354 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  355 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  356 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  357 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  358 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  359 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  360 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  361 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  362 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  363 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  364 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  365 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  366 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  367 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  368 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  369 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  370 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  371 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  372 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  373 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  374 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  375 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  376 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  377 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  378 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  379 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  380 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  381 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  382 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  383 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  384 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  385 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  386 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  387 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  388 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  389 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  390 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  391 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  392 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  393 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  394 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  395 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  396 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  397 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  398 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  399 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  400 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  401 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  402 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  403 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  404 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  405 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  406 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  407 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  408 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  409 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  410 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  411 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  412 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  413 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  414 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  415 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  416 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  417 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  418 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  419 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  420 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  421 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  422 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  423 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  424 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  425 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  426 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  427 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  428 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  429 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  430 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  431 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  432 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  433 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  434 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  435 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  436 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  437 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  438 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  439 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  440 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  441 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  442 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  443 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  444 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  445 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  446 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  447 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  448 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  449 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  450 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  451 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  452 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  453 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  454 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  455 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  456 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  457 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  458 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  459 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  460 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  461 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  462 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  463 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  464 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  465 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  466 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  467 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  468 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  469 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  470 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  471 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  472 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  473 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  474 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  475 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  476 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  477 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  478 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  479 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  480 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  481 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  482 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  483 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  484 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  485 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  486 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  487 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  488 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  489 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  490 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  491 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  492 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  493 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  494 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  495 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  496 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  497 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  498 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  499 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  500 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  501 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  502 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  503 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  504 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  505 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  506 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  507 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  508 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  509 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  510 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  511 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  512 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  513 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  514 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  515 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  516 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  517 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  518 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  519 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  520 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  521 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  522 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  523 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  524 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  525 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  526 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  527 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  528 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  529 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  530 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  531 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  532 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  533 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  534 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  535 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  536 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  537 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  538 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  539 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  540 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  541 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  542 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  543 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  544 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  545 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  546 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  547 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  548 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  549 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  550 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  551 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  552 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  553 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  554 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  555 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  556 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  557 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  558 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  559 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  560 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  561 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  562 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  563 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  564 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  565 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  566 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  567 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  568 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  569 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  570 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  571 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  572 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  573 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  574 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  575 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  576 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  577 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  578 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  579 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  580 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  581 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  582 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  583 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  584 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  585 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  586 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  587 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  588 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  589 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  590 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  591 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  592 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  593 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  594 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  595 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  596 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  597 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  598 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  599 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  600 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  601 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  602 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  603 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  604 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  605 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  606 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  607 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  608 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  609 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  610 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  611 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  612 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  613 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  614 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  615 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  616 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  617 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  618 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  619 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  620 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  621 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  622 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  623 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  624 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  625 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  626 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  627 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  628 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  629 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  630 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  631 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  632 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  633 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  634 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  635 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  636 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  637 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  638 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  639 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  640 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  641 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  642 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  643 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  644 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  645 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  646 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  647 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  648 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  649 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  650 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  651 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  652 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  653 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  654 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  655 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  656 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  657 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  658 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  659 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  660 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  661 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  662 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  663 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  664 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  665 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  666 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  667 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  668 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  669 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  670 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  671 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  672 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  673 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  674 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  675 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  676 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  677 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  678 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  679 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  680 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  681 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  682 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  683 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  684 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  685 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  686 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  687 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  688 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  689 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  690 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  691 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  692 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  693 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  694 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  695 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  696 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  697 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  698 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  699 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  700 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  701 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  702 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  703 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  704 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  705 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  706 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  707 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  708 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  709 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  710 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  711 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  712 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  713 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  714 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  715 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  716 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  717 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  718 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  719 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  720 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  721 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  722 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  723 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  724 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  725 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  726 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  727 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  728 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  729 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  730 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  731 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  732 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  733 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  734 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  735 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  736 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  737 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  738 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  739 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  740 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  741 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  742 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  743 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  744 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  745 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  746 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  747 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  748 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  749 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  750 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  751 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  752 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  753 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  754 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  755 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  756 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  757 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  758 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  759 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  760 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  761 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  762 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  763 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  764 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  765 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  766 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  767 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  768 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  769 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  770 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  771 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  772 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  773 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  774 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  775 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  776 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  777 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  778 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  779 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  780 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  781 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  782 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  783 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  784 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  785 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  786 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  787 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  788 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  789 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  790 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  791 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  792 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  793 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  794 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  795 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  796 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  797 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  798 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  799 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  800 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  801 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  802 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  803 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  804 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  805 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  806 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  807 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  808 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  809 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  810 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  811 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  812 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  813 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  814 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  815 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  816 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  817 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  818 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  819 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  820 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  821 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  822 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  823 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  824 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  825 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  826 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  827 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  828 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  829 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  830 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  831 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  832 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  833 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  834 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  835 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  836 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  837 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  838 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  839 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  840 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  841 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  842 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  843 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  844 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  845 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  846 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  847 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  848 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  849 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  850 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  851 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  852 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  853 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  854 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  855 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  856 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  857 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  858 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  859 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  860 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  861 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  862 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  863 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  864 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  865 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  866 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  867 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  868 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  869 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  870 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  871 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  872 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  873 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  874 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  875 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  876 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  877 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  878 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  879 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  880 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  881 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  882 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  883 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  884 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  885 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  886 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  887 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  888 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  889 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  890 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  891 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  892 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  893 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  894 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  895 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  896 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  897 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  898 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  899 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  900 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  901 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  902 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  903 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  904 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  905 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  906 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  907 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  908 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  909 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  910 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  911 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  912 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  913 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  914 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  915 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  916 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  917 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  918 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  919 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  920 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  921 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  922 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  923 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  924 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  925 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  926 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  927 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  928 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  929 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  930 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  931 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  932 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  933 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  934 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  935 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  936 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  937 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  938 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  939 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  940 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  941 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  942 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  943 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  944 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  945 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  946 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  947 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  948 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  949 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  950 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  951 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  952 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  953 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  954 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  955 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  956 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  957 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  958 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  959 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  960 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  961 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  962 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  963 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  964 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  965 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  966 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  967 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  968 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  969 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  970 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  971 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  972 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  973 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  974 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  975 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  976 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  977 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  978 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  979 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  980 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  981 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  982 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  983 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  984 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  985 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  986 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  987 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  988 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  989 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  990 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  991 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  992 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  993 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  994 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  995 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  996 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  997 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  998 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  999 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1000 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1001 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1002 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1003 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1004 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1005 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1006 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1007 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1008 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1009 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1010 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1011 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1012 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1013 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1014 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1015 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1016 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1017 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1018 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1019 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1020 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1021 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1022 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1023 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1024 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1025 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1026 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1027 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1028 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1029 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1030 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1031 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1032 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1033 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1034 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1035 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1036 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1037 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1038 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1039 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1040 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1041 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1042 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1043 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1044 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1045 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1046 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1047 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1048 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1049 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1050 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1051 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1052 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1053 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1054 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1055 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1056 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1057 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1058 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1059 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1060 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1061 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1062 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1063 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1064 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1065 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1066 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1067 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1068 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1069 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1070 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1071 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1072 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1073 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1074 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1075 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1076 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1077 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1078 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1079 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1080 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1081 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1082 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1083 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1084 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1085 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1086 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1087 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1088 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1089 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1090 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1091 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1092 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1093 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1094 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1095 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1096 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1097 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1098 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1099 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1100 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1101 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1102 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1103 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1104 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1105 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1106 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1107 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1108 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1109 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1110 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1111 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1112 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1113 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1114 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1115 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1116 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1117 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1118 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1119 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1120 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1121 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1122 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1123 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1124 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1125 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1126 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1127 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1128 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1129 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1130 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1131 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1132 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1133 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1134 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1135 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1136 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1137 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1138 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1139 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1140 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1141 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1142 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1143 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1144 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1145 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1146 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1147 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1148 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1149 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1150 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1151 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1152 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1153 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1154 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1155 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1156 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1157 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1158 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1159 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1160 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1161 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1162 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1163 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1164 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1165 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1166 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1167 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1168 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1169 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1170 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1171 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1172 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1173 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1174 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1175 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1176 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1177 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1178 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1179 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1180 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1181 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1182 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1183 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1184 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1185 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1186 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1187 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1188 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1189 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1190 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1191 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1192 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1193 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1194 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1195 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1196 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1197 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1198 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1199 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1200 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1201 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1202 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1203 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1204 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1205 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1206 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1207 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1208 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1209 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1210 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1211 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1212 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1213 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1214 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1215 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1216 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1217 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1218 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1219 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1220 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1221 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1222 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1223 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1224 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1225 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1226 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1227 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1228 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1229 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1230 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1231 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1232 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1233 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1234 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1235 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1236 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1237 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1238 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1239 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1240 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1241 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1242 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1243 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1244 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1245 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1246 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1247 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1248 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1249 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1250 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1251 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1252 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1253 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1254 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1255 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1256 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1257 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1258 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1259 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1260 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1261 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1262 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1263 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1264 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1265 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1266 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1267 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1268 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1269 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1270 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1271 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1272 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1273 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1274 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1275 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1276 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1277 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1278 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1279 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1280 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1281 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1282 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1283 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1284 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1285 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1286 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1287 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1288 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1289 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1290 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1291 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1292 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1293 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1294 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1295 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1296 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1297 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1298 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1299 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1300 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1301 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1302 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1303 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1304 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1305 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1306 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1307 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1308 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1309 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1310 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1311 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1312 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1313 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1314 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1315 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1316 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1317 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1318 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1319 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1320 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1321 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1322 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1323 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1324 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1325 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1326 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1327 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1328 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1329 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1330 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1331 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1332 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1333 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1334 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1335 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1336 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1337 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1338 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1339 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1340 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1341 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1342 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1343 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1344 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1345 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1346 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1347 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1348 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1349 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1350 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1351 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1352 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1353 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1354 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1355 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1356 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1357 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1358 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1359 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1360 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1361 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1362 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1363 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1364 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1365 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1366 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1367 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1368 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1369 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1370 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1371 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1372 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1373 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1374 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1375 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1376 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1377 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1378 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1379 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1380 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1381 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1382 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1383 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1384 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1385 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1386 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1387 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1388 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1389 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1390 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1391 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1392 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1393 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1394 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1395 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1396 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1397 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1398 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1399 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1400 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1401 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1402 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1403 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1404 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1405 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1406 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1407 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1408 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1409 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1410 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1411 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1412 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1413 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1414 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1415 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1416 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1417 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1418 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1419 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1420 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1421 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1422 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1423 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1424 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1425 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1426 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1427 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1428 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1429 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1430 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1431 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1432 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1433 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1434 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1435 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1436 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1437 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1438 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1439 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1440 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1441 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1442 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1443 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1444 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1445 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1446 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1447 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1448 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1449 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1450 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1451 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1452 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1453 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1454 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1455 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1456 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1457 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1458 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1459 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1460 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1461 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1462 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1463 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1464 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1465 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1466 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1467 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1468 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1469 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1470 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1471 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1472 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1473 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1474 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1475 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1476 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1477 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1478 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1479 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1480 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1481 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1482 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1483 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1484 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1485 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1486 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1487 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1488 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1489 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1490 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1491 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1492 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1493 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1494 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1495 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1496 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1497 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1498 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1499 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1500 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1501 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1502 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1503 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1504 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1505 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1506 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1507 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1508 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1509 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1510 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1511 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1512 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1513 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1514 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1515 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1516 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1517 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1518 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1519 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1520 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1521 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1522 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1523 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1524 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1525 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1526 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1527 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1528 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1529 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1530 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1531 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1532 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1533 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1534 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1535 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1536 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1537 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1538 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1539 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1540 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1541 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1542 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1543 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1544 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1545 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1546 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1547 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1548 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1549 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1550 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1551 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1552 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1553 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1554 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1555 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1556 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1557 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1558 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1559 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1560 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1561 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1562 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1563 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1564 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1565 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1566 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1567 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1568 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1569 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1570 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1571 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1572 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1573 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1574 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1575 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1576 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1577 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1578 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1579 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1580 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1581 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1582 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1583 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1584 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1585 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1586 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1587 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1588 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1589 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1590 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1591 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1592 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1593 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1594 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1595 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1596 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1597 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1598 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1599 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1600 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1601 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1602 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1603 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1604 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1605 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1606 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1607 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1608 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1609 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1610 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1611 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1612 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1613 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1614 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1615 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1616 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1617 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1618 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1619 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1620 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1621 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1622 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1623 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1624 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1625 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1626 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1627 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1628 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1629 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1630 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1631 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1632 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1633 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1634 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1635 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1636 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1637 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1638 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1639 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1640 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1641 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1642 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1643 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1644 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1645 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1646 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1647 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1648 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1649 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1650 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1651 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1652 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1653 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1654 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1655 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1656 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1657 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1658 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1659 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1660 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1661 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1662 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1663 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1664 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1665 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1666 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1667 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1668 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1669 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1670 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1671 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1672 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1673 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1674 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1675 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1676 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1677 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1678 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1679 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1680 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1681 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1682 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1683 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1684 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1685 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1686 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1687 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1688 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1689 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1690 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1691 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1692 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1693 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1694 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1695 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1696 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1697 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1698 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1699 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1700 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1701 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1702 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1703 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1704 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1705 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1706 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1707 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1708 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1709 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1710 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1711 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1712 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1713 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1714 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1715 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1716 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1717 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1718 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1719 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1720 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1721 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1722 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1723 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1724 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1725 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1726 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1727 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1728 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1729 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1730 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1731 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1732 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1733 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1734 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1735 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1736 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1737 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1738 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1739 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1740 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1741 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1742 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1743 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1744 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1745 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1746 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1747 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1748 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1749 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1750 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1751 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1752 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1753 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1754 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1755 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1756 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1757 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1758 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1759 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1760 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1761 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1762 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1763 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1764 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1765 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1766 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1767 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1768 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1769 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1770 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1771 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1772 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1773 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1774 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1775 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1776 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1777 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1778 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1779 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1780 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1781 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1782 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1783 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1784 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1785 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1786 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1787 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1788 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1789 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1790 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1791 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1792 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1793 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1794 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1795 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1796 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1797 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1798 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1799 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1800 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1801 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1802 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1803 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1804 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1805 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1806 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1807 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1808 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1809 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1810 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1811 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1812 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1813 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1814 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1815 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1816 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1817 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1818 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1819 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1820 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1821 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1822 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1823 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1824 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1825 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1826 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1827 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1828 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1829 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1830 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1831 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1832 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1833 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1834 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1835 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1836 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1837 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1838 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1839 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1840 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1841 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1842 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1843 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1844 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1845 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1846 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1847 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1848 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1849 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1850 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1851 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1852 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1853 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1854 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1855 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1856 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1857 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1858 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1859 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1860 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1861 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1862 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1863 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1864 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1865 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1866 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1867 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1868 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1869 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1870 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1871 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1872 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1873 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1874 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1875 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1876 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1877 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1878 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1879 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1880 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1881 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1882 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1883 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1884 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1885 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1886 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1887 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1888 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1889 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1890 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1891 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1892 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1893 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1894 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1895 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1896 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1897 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1898 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1899 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1900 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1901 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1902 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1903 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1904 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1905 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1906 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1907 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1908 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1909 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1910 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1911 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1912 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1913 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1914 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1915 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1916 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1917 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1918 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1919 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1920 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1921 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1922 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1923 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1924 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1925 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1926 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1927 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1928 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1929 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1930 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1931 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1932 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1933 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1934 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1935 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1936 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1937 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1938 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1939 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1940 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1941 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1942 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1943 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1944 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1945 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1946 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1947 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1948 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1949 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1950 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1951 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1952 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1953 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1954 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1955 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1956 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1957 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1958 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1959 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1960 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1961 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1962 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1963 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1964 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1965 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1966 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1967 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1968 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1969 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1970 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1971 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1972 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1973 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1974 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1975 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1976 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1977 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1978 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1979 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1980 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1981 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1982 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1983 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1984 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1985 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1986 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1987 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1988 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1989 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1990 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1991 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1992 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1993 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1994 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1995 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1996 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1997 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1998 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1999 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2000 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2001 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2002 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2003 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2004 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2005 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2006 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2007 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2008 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2009 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2010 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2011 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2012 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2013 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2014 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2015 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2016 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2017 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2018 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2019 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2020 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2021 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2022 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2023 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2024 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2025 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2026 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2027 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2028 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2029 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2030 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2031 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2032 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2033 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2034 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2035 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2036 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2037 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2038 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2039 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2040 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2041 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2042 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2043 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2044 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2045 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2046 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2047 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2048 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2049 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2050 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2051 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2052 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2053 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2054 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2055 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2056 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2057 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2058 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2059 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2060 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2061 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2062 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2063 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2064 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2065 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2066 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2067 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2068 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2069 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2070 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2071 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2072 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2073 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2074 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2075 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2076 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2077 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2078 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2079 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2080 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2081 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2082 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2083 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2084 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2085 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2086 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2087 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2088 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2089 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2090 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2091 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2092 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2093 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2094 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2095 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2096 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2097 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2098 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2099 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2100 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2101 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2102 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2103 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2104 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2105 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2106 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2107 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2108 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2109 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2110 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2111 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2112 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2113 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2114 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2115 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2116 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2117 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2118 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2119 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2120 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2121 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2122 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2123 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2124 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2125 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2126 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2127 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2128 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2129 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2130 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2131 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2132 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2133 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2134 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2135 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2136 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2137 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2138 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2139 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2140 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2141 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2142 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2143 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2144 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2145 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2146 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2147 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2148 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2149 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2150 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2151 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2152 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2153 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2154 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2155 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2156 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2157 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2158 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2159 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2160 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2161 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2162 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2163 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2164 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2165 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2166 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2167 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2168 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2169 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2170 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2171 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2172 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2173 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2174 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2175 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2176 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2177 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2178 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2179 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2180 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2181 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2182 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2183 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2184 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2185 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2186 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2187 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2188 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2189 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2190 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2191 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2192 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2193 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2194 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2195 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2196 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2197 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2198 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2199 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2200 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2201 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2202 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2203 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2204 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2205 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2206 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2207 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2208 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2209 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2210 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2211 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2212 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2213 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2214 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2215 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2216 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2217 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2218 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2219 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2220 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2221 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2222 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2223 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2224 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2225 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2226 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2227 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2228 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2229 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2230 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2231 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2232 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2233 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2234 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2235 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2236 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2237 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2238 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2239 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2240 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2241 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2242 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2243 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2244 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2245 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2246 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2247 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2248 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2249 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2250 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2251 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2252 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2253 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2254 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2255 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2256 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2257 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2258 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2259 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2260 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2261 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2262 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2263 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2264 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2265 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2266 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2267 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2268 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2269 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2270 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2271 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2272 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2273 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2274 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2275 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2276 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2277 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2278 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2279 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2280 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2281 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2282 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2283 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2284 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2285 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2286 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2287 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2288 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2289 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2290 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2291 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2292 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2293 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2294 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2295 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2296 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2297 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2298 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2299 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2300 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2301 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2302 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2303 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2304 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2305 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2306 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2307 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2308 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2309 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2310 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2311 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2312 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2313 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2314 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2315 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2316 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2317 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2318 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2319 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2320 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2321 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2322 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2323 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2324 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2325 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2326 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2327 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2328 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2329 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2330 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2331 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2332 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2333 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2334 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2335 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2336 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2337 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2338 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2339 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2340 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2341 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2342 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2343 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2344 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2345 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2346 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2347 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2348 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2349 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2350 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2351 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2352 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2353 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2354 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2355 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2356 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2357 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2358 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2359 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2360 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2361 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2362 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2363 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2364 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2365 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2366 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2367 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2368 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2369 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2370 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2371 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2372 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2373 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2374 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2375 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2376 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2377 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2378 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2379 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2380 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2381 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2382 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2383 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2384 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2385 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2386 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2387 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2388 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2389 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2390 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2391 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2392 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2393 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2394 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2395 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2396 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2397 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2398 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2399 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2400 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2401 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2402 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2403 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2404 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2405 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2406 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2407 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2408 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2409 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2410 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2411 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2412 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2413 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2414 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2415 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2416 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2417 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2418 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2419 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2420 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2421 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2422 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2423 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2424 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2425 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2426 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2427 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2428 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2429 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2430 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2431 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2432 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2433 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2434 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2435 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2436 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2437 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2438 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2439 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2440 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2441 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2442 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2443 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2444 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2445 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2446 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2447 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2448 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2449 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2450 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2451 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2452 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2453 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2454 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2455 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2456 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2457 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2458 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2459 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2460 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2461 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2462 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2463 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2464 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2465 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2466 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2467 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2468 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2469 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2470 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2471 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2472 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2473 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2474 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2475 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2476 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2477 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2478 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2479 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2480 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2481 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2482 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2483 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2484 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2485 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2486 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2487 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2488 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2489 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2490 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2491 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2492 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2493 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2494 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2495 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2496 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2497 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2498 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2499 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2500 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2501 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2502 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2503 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2504 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2505 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2506 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2507 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2508 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2509 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2510 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2511 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2512 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2513 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2514 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2515 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2516 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2517 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2518 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2519 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2520 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2521 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2522 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2523 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2524 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2525 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2526 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2527 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2528 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2529 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2530 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2531 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2532 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2533 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2534 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2535 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2536 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2537 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2538 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2539 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2540 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2541 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2542 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2543 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2544 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2545 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2546 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2547 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2548 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2549 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2550 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2551 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2552 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2553 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2554 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2555 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2556 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2557 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2558 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2559 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2560 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2561 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2562 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2563 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2564 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2565 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2566 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2567 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2568 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2569 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2570 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2571 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2572 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2573 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2574 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2575 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2576 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2577 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2578 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2579 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2580 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2581 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2582 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2583 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2584 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2585 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2586 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2587 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2588 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2589 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2590 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2591 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2592 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2593 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2594 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2595 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2596 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2597 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2598 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2599 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2600 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2601 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2602 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2603 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2604 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2605 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2606 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2607 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2608 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2609 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2610 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2611 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2612 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2613 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2614 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2615 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2616 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2617 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2618 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2619 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2620 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2621 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2622 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2623 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2624 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2625 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2626 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2627 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2628 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2629 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2630 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2631 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2632 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2633 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2634 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2635 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2636 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2637 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2638 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2639 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2640 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2641 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2642 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2643 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2644 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2645 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2646 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2647 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2648 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2649 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2650 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2651 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2652 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2653 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2654 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2655 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2656 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2657 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2658 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2659 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2660 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2661 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2662 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2663 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2664 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2665 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2666 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2667 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2668 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2669 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2670 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2671 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2672 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2673 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2674 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2675 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2676 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2677 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2678 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2679 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2680 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2681 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2682 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2683 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2684 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2685 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2686 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2687 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2688 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2689 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2690 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2691 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2692 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2693 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2694 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2695 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2696 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2697 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2698 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2699 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2700 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2701 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2702 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2703 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2704 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2705 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2706 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2707 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2708 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2709 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2710 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2711 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2712 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2713 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2714 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2715 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2716 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2717 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2718 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2719 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2720 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2721 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2722 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2723 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2724 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2725 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2726 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2727 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2728 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2729 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2730 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2731 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2732 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2733 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2734 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2735 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2736 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2737 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2738 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2739 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2740 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2741 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2742 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2743 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2744 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2745 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2746 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2747 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2748 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2749 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2750 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2751 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2752 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2753 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2754 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2755 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2756 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2757 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2758 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2759 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2760 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2761 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2762 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2763 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2764 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2765 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2766 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2767 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2768 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2769 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2770 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2771 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2772 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2773 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2774 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2775 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2776 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2777 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2778 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2779 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2780 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2781 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2782 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2783 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2784 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2785 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2786 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2787 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2788 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2789 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2790 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2791 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2792 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2793 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2794 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2795 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2796 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2797 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2798 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2799 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2800 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2801 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2802 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2803 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2804 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2805 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2806 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2807 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2808 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2809 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2810 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2811 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2812 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2813 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2814 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2815 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2816 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2817 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2818 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2819 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2820 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2821 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2822 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2823 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2824 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2825 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2826 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2827 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2828 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2829 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2830 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2831 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2832 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2833 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2834 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2835 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2836 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2837 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2838 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2839 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2840 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2841 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2842 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2843 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2844 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2845 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2846 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2847 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2848 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2849 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2850 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2851 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2852 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2853 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2854 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2855 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2856 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2857 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2858 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2859 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2860 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2861 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2862 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2863 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2864 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2865 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2866 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2867 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2868 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2869 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2870 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2871 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2872 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2873 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2874 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2875 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2876 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2877 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2878 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2879 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2880 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2881 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2882 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2883 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2884 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2885 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2886 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2887 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2888 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2889 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2890 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2891 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2892 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2893 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2894 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2895 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2896 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2897 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2898 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2899 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2900 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2901 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2902 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2903 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2904 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2905 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2906 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2907 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2908 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2909 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2910 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2911 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2912 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2913 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2914 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2915 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2916 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2917 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2918 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2919 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2920 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2921 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2922 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2923 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2924 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2925 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2926 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2927 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2928 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2929 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2930 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2931 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2932 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2933 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2934 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2935 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2936 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2937 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2938 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2939 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2940 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2941 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2942 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2943 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2944 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2945 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2946 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2947 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2948 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2949 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2950 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2951 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2952 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2953 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2954 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2955 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2956 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2957 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2958 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2959 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2960 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2961 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2962 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2963 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2964 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2965 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2966 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2967 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2968 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2969 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2970 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2971 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2972 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2973 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2974 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2975 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2976 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2977 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2978 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2979 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2980 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2981 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2982 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2983 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2984 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2985 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2986 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2987 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2988 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2989 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2990 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2991 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2992 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2993 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2994 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  2995 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  2996 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2997 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  2998 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  2999 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3000 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3001 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3002 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3003 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3004 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3005 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3006 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3007 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3008 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3009 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3010 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3011 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3012 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3013 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3014 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3015 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3016 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3017 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3018 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3019 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3020 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3021 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3022 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3023 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3024 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3025 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3026 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3027 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3028 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3029 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3030 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3031 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3032 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3033 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3034 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3035 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3036 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3037 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3038 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3039 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3040 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3041 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3042 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3043 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3044 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3045 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3046 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3047 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3048 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3049 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3050 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3051 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3052 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3053 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3054 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3055 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3056 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3057 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3058 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3059 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3060 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3061 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3062 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3063 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3064 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3065 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3066 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3067 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3068 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3069 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3070 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3071 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3072 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3073 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3074 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3075 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3076 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3077 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3078 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3079 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3080 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3081 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3082 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3083 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3084 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3085 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3086 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3087 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3088 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3089 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3090 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3091 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3092 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3093 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3094 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3095 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3096 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3097 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3098 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3099 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3100 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3101 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3102 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3103 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3104 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3105 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3106 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3107 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3108 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3109 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3110 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3111 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3112 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3113 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3114 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3115 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3116 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3117 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3118 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3119 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3120 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3121 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3122 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3123 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3124 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3125 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3126 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3127 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3128 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3129 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3130 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3131 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3132 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3133 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3134 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3135 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3136 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3137 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3138 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3139 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3140 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3141 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3142 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3143 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3144 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3145 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3146 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3147 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3148 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3149 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3150 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3151 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3152 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3153 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3154 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3155 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3156 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3157 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3158 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3159 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3160 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3161 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3162 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3163 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3164 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3165 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3166 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3167 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3168 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3169 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3170 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3171 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3172 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3173 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3174 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3175 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3176 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3177 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3178 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3179 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.01, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3180 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3181 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3182 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3183 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3184 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3185 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3186 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3187 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3188 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3189 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3190 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3191 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3192 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3193 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3194 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3195 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3196 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3197 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3198 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3199 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3200 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3201 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3202 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3203 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3204 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3205 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3206 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3207 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3208 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3209 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3210 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3211 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3212 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3213 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3214 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3215 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3216 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3217 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3218 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3219 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3220 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3221 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3222 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3223 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3224 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3225 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3226 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3227 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3228 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3229 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3230 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3231 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3232 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3233 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3234 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3235 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  3236 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  3237 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3238 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  3239 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 100, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  3240 th iteration\n",
      "Best Score: 0.001867078710347414\n",
      "Best Parameters: {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.01, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}}\n"
     ]
    }
   ],
   "source": [
    "### Build a RNN model testing different parameters\n",
    "\n",
    "# Function to build the model\n",
    "def build_model(n_layers = 2, # Number of hidden layers\n",
    "                optimizer='adam', # Optimizer\n",
    "                learning_rate=0.001, # Learning rate of the optimizer\n",
    "                rnn_units={0: 50,  1: 20, 2: 10}, # Number of units in each layer\n",
    "                #alphas_l1_l2=0.01, # Removed because it didn't improve the model\n",
    "                dropout_rate=0.2 # Dropout rate to avoid overfitting\n",
    "                ):\n",
    "    # Check the optimizer\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    # Define the input layer shape\n",
    "    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    # Add the RNN layers\n",
    "    for i in range(n_layers):\n",
    "        if i < n_layers - 1:  # intermediate hidden layers\n",
    "            model.add(SimpleRNN(units=rnn_units[i+1], \n",
    "                                #kernel_regularizer=l1_l2(l1=alphas_l1_l2, l2=alphas_l1_l2), # Removed because it didn't improve the model\n",
    "                                #recurrent_regularizer=l1_l2(l1=alphas_l1_l2, l2=alphas_l1_l2), # Removed because it didn't improve the model\n",
    "                                #bias_regularizer=l1_l2(l1=alphas_l1_l2, l2=alphas_l1_l2), # Removed because it didn't improve the model\n",
    "                                return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        else:  # last hidden layer\n",
    "            model.add(SimpleRNN(units=rnn_units[i+1],  \n",
    "                                #kernel_regularizer=l1_l2(l1=alphas_l1_l2, l2=alphas_l1_l2), # Removed because it didn't improve the model\n",
    "                                #recurrent_regularizer=l1_l2(l1=alphas_l1_l2, l2=alphas_l1_l2), # Removed because it didn't improve the model\n",
    "                                #bias_regularizer=l1_l2(l1=alphas_l1_l2, l2=alphas_l1_l2), # Removed because it didn't improve the model\n",
    "                                return_sequences=False))\n",
    "            model.add(Dropout(dropout_rate)) # Dropout layer to avoid overfitting\n",
    "    model.add(Dense(units=1)) #output layer\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Function to test each set of parameters\n",
    "def custom_fit(params):\n",
    "    training_params = {key: params[key] for key in params if key in ['n_layers', \n",
    "                                                                     #'alphas_l1_l2', \n",
    "                                                                     'dropout_rate',\n",
    "                                                                     'rnn_units', \n",
    "                                                                     'optimizer', \n",
    "                                                                     'learning_rate']} # Use this to pass the parameters to the model\n",
    "    model = build_model(**training_params) # Create the model using the parameters\n",
    "    \n",
    "    # Define EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True) # Stop training when the validation loss is no longer decreasing after X epochs\n",
    "\n",
    "    # Fitting the model with early stopping\n",
    "    model.fit(X_train, Y_train, \n",
    "              epochs=params['epochs'], \n",
    "              batch_size=params['batch_size'], \n",
    "              verbose=0,\n",
    "              validation_data=(X_val, Y_val), \n",
    "              callbacks=[early_stopping])\n",
    "    # Compute the loss on the validation set\n",
    "    loss = model.evaluate(X_val, Y_val, verbose=0)\n",
    "    return loss\n",
    "\n",
    "# Parameters to test with possible coefficients\n",
    "param_grid = {\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    #'alphas_l1_l2' : [0.001, 0.01, 0.1, 1], # Removed because it didn't improve the model\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'epochs': [20, 50, 75, 100],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001], # Learning rate\n",
    "    'n_layers': [1,2,3], # Number of hidden layers\n",
    "    'rnn_units' : [{1: 10,  2: 10, 3: 5},\n",
    "                   {1: 50,  2: 30, 3: 15},\n",
    "                   {1: 100,  2: 100, 3: 50}, {1: 200,  2: 200, 3: 100}, {1: 400,  2: 300, 3: 100}]  \n",
    "}\n",
    "\n",
    "# Compare the scores for each set of parameters\n",
    "best_score = np.inf # Initialize the best score object\n",
    "best_params = None # Initialize the best parameters object\n",
    "interactions = 1 # Initialize the interactions counter\n",
    "for params in ParameterGrid(param_grid): # Loop through the parameters\n",
    "    # Print the parameters and the iteration to keep track of the progress\n",
    "    print(\"Applying parameters: \", params,\". This is the \", interactions, \"th iteration\")\n",
    "    interactions += 1 # increment the interactions counter\n",
    "    score = custom_fit(params) # Compute the score\n",
    "    if score < best_score: # Check if the score is better than the best score\n",
    "        best_score = score # Update the best score\n",
    "        best_params = params # Update the best parameters\n",
    "\n",
    "# Print the best score and the best parameters\n",
    "print(f'Best Score: {best_score}')\n",
    "print(f'Best Parameters: {best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d59d3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's build the model with the best parameters and train it\n",
    "# Get epochs and batch_size from best_params\n",
    "best_params_bkp = best_params.copy() # Just to keep a backup of the best parameters\n",
    "epochs = best_params.pop('epochs') # Remove the epochs from the best parameters\n",
    "batch_size = best_params.pop('batch_size') # Remove the batch_size from the best parameters\n",
    "\n",
    "# build the model\n",
    "best_model = build_model(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a90bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model. Therefore we can use it later\n",
    "best_model.save('models_parameters/best_rnn_model_grid_Lasso_annual_10.keras')  # Saves the model to a keras file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "566c1b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #If we need to restore the best model\n",
    "\n",
    "# # Load the model from the file\n",
    "# best_model = load_model('models_parameters/best_rnn_model_grid_lasso_10.keras')\n",
    "# # from manual hyperparameter tuning\n",
    "# epochs = 75\n",
    "# batch_size = 64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "544e7993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 601ms/step\n"
     ]
    }
   ],
   "source": [
    "# Now let's train the model and make predictions on validation set\n",
    "\n",
    "# train the model\n",
    "history = best_model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "predictions_scaled = best_model.predict(X_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62bd6363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and validation loss values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "016ae6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE RNN: 0.4097212851047516\n",
      "RMSE RNN: 44987.56640625\n",
      "MAE RNN: 43768.203125\n"
     ]
    }
   ],
   "source": [
    "# Let's reshape the predictions and Y_val to revert the scaling\n",
    "# Reshape predictions to 2D\n",
    "predictions_scaled_2d = predictions_scaled.reshape(-1, 1)\n",
    "# Get the last timestep of X_val\n",
    "X_val_last_timestep = X_val[:, -1, :]\n",
    "# Replace the first column of X_val_last_timestep with the scaled predictions.\n",
    "X_val_last_timestep[:, 0] = predictions_scaled_2d[:, 0]\n",
    "# unscale the predictions\n",
    "predictions_rescaled = scaler.inverse_transform(X_val_last_timestep)[:, 0]\n",
    "\n",
    "# unscale the Y_val\n",
    "Y_val_rescaled = scaler.inverse_transform(val.iloc[-len(predictions_scaled):, :].values)[:, 0]\n",
    "\n",
    "# Calculate the validation error\n",
    "mape_RNN_val = mean_absolute_percentage_error(Y_val_rescaled, predictions_rescaled)\n",
    "rmse_RNN_val = np.sqrt(mean_squared_error(Y_val_rescaled, predictions_rescaled))\n",
    "mae_RNN_val = mean_absolute_error(Y_val_rescaled, predictions_rescaled)\n",
    "\n",
    "print(f'MAPE RNN: {mape_RNN_val}')\n",
    "print(f'RMSE RNN: {rmse_RNN_val}')\n",
    "print(f'MAE RNN: {mae_RNN_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a80f71c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n"
     ]
    }
   ],
   "source": [
    "# Let's predict the test set using the best model\n",
    "predictions_test_scaled = best_model.predict(X_test)\n",
    "\n",
    "# Let's reshape the predictions and Y_val to revert the scaling\n",
    "# Reshape predictions to 2D\n",
    "predictions_test_scaled_2d = predictions_test_scaled.reshape(-1, 1)\n",
    "# Get the last timestep of X_test\n",
    "X_test_last_timestep = X_test[:, -1, :]\n",
    "# Replace the first column of X_test_last_timestep with the scaled predictions.\n",
    "X_test_last_timestep[:, 0] = predictions_test_scaled_2d[:, 0]\n",
    "# unscale the predictions\n",
    "predictions_test_rescaled = scaler.inverse_transform(X_test_last_timestep)[:, 0]\n",
    "\n",
    "# Let's convert the predictions and Y_test to a dataframe usind the index from test\n",
    "predictions_test_df = pd.DataFrame(predictions_test_rescaled, index=test.index[-len(predictions_test_rescaled):], columns=[target_variable])\n",
    "predictions = predictions_test_df.copy()\n",
    "\n",
    "# Get the original Y_test\n",
    "Y_test = df_adjusted[-len(predictions):][target_variable]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5a61945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE best RNN: 9.19607162475586\n",
      "RMSE best RNN: 118297.5\n",
      "MAE best RNN: 105905.8203125\n"
     ]
    }
   ],
   "source": [
    "# Calculate the error on the test set\n",
    "mape_best_RNN = mean_absolute_percentage_error(Y_test, predictions)\n",
    "rmse_best_RNN = np.sqrt(mean_squared_error(Y_test, predictions))\n",
    "mae_best_RNN = mean_absolute_error(Y_test, predictions)\n",
    "\n",
    "print(f'MAPE best RNN: {mape_best_RNN}')\n",
    "print(f'RMSE best RNN: {rmse_best_RNN}')\n",
    "print(f'MAE best RNN: {mae_best_RNN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot RNN best Model predictions vs Actuals - Test Set\n",
    "\n",
    "# Convert Y_test_rescaled to a dataframe using the test index\n",
    "Y_test_rescaled_df = pd.DataFrame(Y_test, index=Y_test.index)\n",
    "# Change the column name to the target variable\n",
    "Y_test_rescaled_df.columns = [target_variable]\n",
    "\n",
    "uf.plot_prediction_vs_test(\n",
    "    target_variable, \n",
    "    Y_test_rescaled_df[target_variable],\n",
    "    predictions,\n",
    "    #predictions_test_rescaled, \n",
    "    'RNN Model Prediction vs Actuals - Test Set',)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
