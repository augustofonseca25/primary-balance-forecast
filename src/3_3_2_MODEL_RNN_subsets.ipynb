{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the code to evaluate the best RNN model against different subset datasets and outliers removal thresholds.\n",
    "#### References:\n",
    "- Peixeiro, M. (2022). Time series forecasting in Python. Manning. Includes the codes fromi its GitHub repo (https://github.com/marcopeix/AppliedTimeSeriesForecastingInPython).   \n",
    "Contribution: The technique for converting the series into sequenced samples and the idea for scale and reeschale data after predictions.\n",
    "- Discolll, N. (2024, January 12). Harnessing RNNs for Financial Time Series Analysis: A Python Approach. Medium. https://medium.com/@redeaddiscolll/harnessing-rnns-for-financial-time-series-analysis-a-python-approach-0669b3a25c7a.   \n",
    "Contribution: EarlyStopping function for the RNN model.\n",
    "\n",
    "#### Libraries\n",
    "- Package Pandas (2.2). (2024). [Python]. https://pandas.pydata.org/\n",
    "- Package NumPy (1.23). (2023). [Pyhton]. https://numpy.org/ - Harris, C. R., Millman, K. J., Van Der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., Van Kerkwijk, M. H., Brett, M., Haldane, A., Del Río, J. F., Wiebe, M., Peterson, P., … Oliphant, T. E. (2020). Array programming with NumPy. Nature, 585(7825), 357–362. https://doi.org/10.1038/s41586-020-2649-2\n",
    "- Droettboom, J. D. H., Michael. (2024). Package matplotlib (3.8.4) [Python]. https://matplotlib.org\n",
    "- Package scikit-learn (1.4). (2024). [Pyhton]. https://scikit-learn.org/stable/index.html\n",
    "- Package Tensorflow (2.16). (2024). [Python]. https://github.com/tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\My Drive\\Data_Projects\\MDS\\master_thesis\\fiscal-balance-forecast\\.venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 10 variables whereas the saved optimizer has 2 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REading File: ../data/data_cleaned_LASSO.csv\n",
      "Outlier Threshold: nan\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step\n",
      "Outlier Threshold: 0.05\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Outlier Threshold: 0.1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Outlier Threshold: 0.15\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Outlier Threshold: 0.2\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "File: ../data/data_cleaned_LASSO.csv, Outlier Threshold: nan -> Errors: {'MAPE': 17.81, 'RMSE': 161587.75, 'MAE': 127844.89}\n",
      "File: ../data/data_cleaned_LASSO.csv, Outlier Threshold: 0.05 -> Errors: {'MAPE': 8.68, 'RMSE': 127069.23, 'MAE': 112575.6}\n",
      "File: ../data/data_cleaned_LASSO.csv, Outlier Threshold: 0.1 -> Errors: {'MAPE': 7.16, 'RMSE': 115088.43, 'MAE': 100662.52}\n",
      "File: ../data/data_cleaned_LASSO.csv, Outlier Threshold: 0.15 -> Errors: {'MAPE': 7.97, 'RMSE': 121827.9, 'MAE': 106456.31}\n",
      "File: ../data/data_cleaned_LASSO.csv, Outlier Threshold: 0.2 -> Errors: {'MAPE': 8.99, 'RMSE': 131438.73, 'MAE': 115978.11}\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (mean_absolute_error,\n",
    "                             mean_absolute_percentage_error,\n",
    "                             mean_squared_error)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, SimpleRNN\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "import useful_functions as uf\n",
    "\n",
    "# Possible datasets to test - uncomment the desired dataset according to the model that will be loaded\n",
    "file_paths = [\n",
    "    #'../data/data_orig_parameters.csv'\n",
    "    #'../data/data_cleaned_RF.csv',\n",
    "    '../data/data_cleaned_LASSO.csv'\n",
    "    #'../data/data_cleaned_RFE.csv'\n",
    "]\n",
    "\n",
    "# List of outlier thresholds to test\n",
    "outlier_thresholds = [np.nan, 0.05, 0.10, 0.15, 0.20]\n",
    "\n",
    "# Dictionary to store the errors\n",
    "errors_dict = {}\n",
    "\n",
    "# Load the model from the file\n",
    "best_model = load_model('models_parameters/best_rnn_model_grid_Lasso_annual_10.keras')\n",
    "\n",
    "# Define the manual hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "patience = 10\n",
    "\n",
    "# Loop through the files and outlier thresholds\n",
    "for file_path in file_paths:\n",
    "    print(f\"REading File: {file_path}\")\n",
    "    for remove_outliers_threshold in outlier_thresholds:\n",
    "        print(f\"Outlier Threshold: {remove_outliers_threshold}\")\n",
    "        \n",
    "        # Load  data\n",
    "        df_raw = pd.read_csv(file_path, parse_dates=['Date'], index_col='Date')\n",
    "        target_variable = df_raw.columns[0] # Set the target variable as the first column of the dataframe\n",
    "        \n",
    "        # Convert all columns to float\n",
    "        df_raw = df_raw.astype('float64')   \n",
    "        df = df_raw.copy()\n",
    "\n",
    "        # Remove outliers using the threshold\n",
    "        if not pd.isna(remove_outliers_threshold): # if the threshold is not NaN\n",
    "            df_cleaned = uf.remove_outliers(df.copy(), threshold=remove_outliers_threshold)\n",
    "        else: # if the threshold is NaN, we will not remove the outliers\n",
    "            df_cleaned = df.copy()\n",
    "\n",
    "        # After removing the outliers, we need to fill the missing values again\n",
    "        df_adjusted = uf.fill_missing_values(df_cleaned) \n",
    "        \n",
    "        # Define test, train and validation set sizes\n",
    "        val_size = 48 # 48 months or 4 years\n",
    "        test_size = 48 # 48 months or 4 years\n",
    "\n",
    "        # Split the data into train and test sets\n",
    "        train_raw_total = df_adjusted[:-test_size] # This total trainning set will be used to train the final model\n",
    "        df_train = train_raw_total[:-val_size]\n",
    "        df_val = train_raw_total[-val_size:]\n",
    "        df_test = df_adjusted[-test_size:]\n",
    "\n",
    "        # Let´s scale the dfs\n",
    "        # Create the scaler\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        scaled_train = scaler.fit_transform(df_train) # Fit the scaler to the train set and transform it\n",
    "        scaled_val = scaler.transform(df_val) # Transform the validation set\n",
    "        scaled_test = scaler.transform(df_test) # Transform the test set\n",
    "        # include df columns names in the train and test sets\n",
    "        train = pd.DataFrame(scaled_train, columns=df_train.columns)\n",
    "        val = pd.DataFrame(scaled_val, columns=df_val.columns)\n",
    "        test = pd.DataFrame(scaled_test, columns=df_test.columns)\n",
    "        # Include the index in the train and test sets\n",
    "        train.index = df_train.index\n",
    "        val.index = df_val.index\n",
    "        test.index = df_test.index\n",
    "\n",
    "        # Converting the series to samples\n",
    "        # We will use the past 12 months to predict the next 12 months\n",
    "        def createXY(dataset, n_past, n_future):\n",
    "            dataX, dataY = [], []\n",
    "            # Loop for the entire dataset\n",
    "            for i in range(n_past, len(dataset) - n_future + 1):\n",
    "                dataX.append(dataset.iloc[i - n_past:i].values)  # Past n months\n",
    "                dataY.append(dataset.iloc[i + n_future - 1, 0])  #\n",
    "            return np.array(dataX), np.array(dataY)\n",
    "\n",
    "        n_past = 12  # Number of past months to use\n",
    "        n_future = 12  # Number of future months to predict\n",
    "\n",
    "        # Create the samples\n",
    "        X_train, Y_train = createXY(train, n_past, n_future)\n",
    "        X_val, Y_val = createXY(val, n_past, n_future)\n",
    "        X_test, Y_test = createXY(test, n_past, n_future)\n",
    "\n",
    "        # Define EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                       patience=patience, \n",
    "                                       restore_best_weights=True) # Stop training when the validation loss is no longer decreasing after X epochs\n",
    "\n",
    "        # train the model\n",
    "        history = best_model.fit(X_train, Y_train,\n",
    "                                 validation_data=(X_val, Y_val),\n",
    "                                 epochs=epochs, batch_size=batch_size,\n",
    "                                 verbose=0,callbacks=[early_stopping])\n",
    "        \n",
    "        # Let's predict the test set using the best model\n",
    "        predictions_test_scaled = best_model.predict(X_test)\n",
    "\n",
    "        # Let's reshape the predictions and Y_val to revert the scaling\n",
    "        # Reshape predictions to 2D\n",
    "        predictions_test_scaled_2d = predictions_test_scaled.reshape(-1, 1)\n",
    "        \n",
    "        # Get the last timestep of X_test\n",
    "        X_test_last_timestep = X_test[:, -1, :]\n",
    "        \n",
    "        # Replace the first column of X_test_last_timestep with the scaled predictions.\n",
    "        X_test_last_timestep[:, 0] = predictions_test_scaled_2d[:, 0]\n",
    "        \n",
    "        # unscale the predictions\n",
    "        predictions_test_rescaled = scaler.inverse_transform(X_test_last_timestep)[:, 0]\n",
    "\n",
    "        # Let's convert the predictions and Y_test to a dataframe usind the index from test\n",
    "        predictions_test_df = pd.DataFrame(predictions_test_rescaled, index=test.index[-len(predictions_test_rescaled):], columns=[target_variable])\n",
    "        predictions = predictions_test_df.copy()\n",
    "        # Get the real values of Y_test to compare with the predictions\n",
    "        Y_test = df_adjusted[-len(predictions):][target_variable]\n",
    "\n",
    "        # Calculate the error\n",
    "        mape_best_RNN = mean_absolute_percentage_error(Y_test, predictions).round(2)\n",
    "        rmse_best_RNN = np.sqrt(mean_squared_error(Y_test, predictions)).round(2)\n",
    "        mae_best_RNN = mean_absolute_error(Y_test, predictions).round(2)\n",
    "\n",
    "        # Store the errors in the dictionary\n",
    "        errors_dict[(file_path, remove_outliers_threshold)] = {'MAPE': mape_best_RNN, 'RMSE': rmse_best_RNN, 'MAE': mae_best_RNN}\n",
    "\n",
    "# Print the errors\n",
    "for key, value in errors_dict.items():\n",
    "    print(f\"File: {key[0]}, Outlier Threshold: {key[1]} -> Errors: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
