{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the code to evaluate the best GRU model against different subset datasets and outliers removal thresholds.\n",
    "#### References:\n",
    "- Peixeiro, M. (2022). Time series forecasting in Python. Manning. Includes the codes fromi its GitHub repo (https://github.com/marcopeix/AppliedTimeSeriesForecastingInPython).   \n",
    "Contribution: The technique for converting the series into sequenced samples and the idea for scale and reeschale data after predictions.\n",
    "- Discolll, N. (2024, January 12). Harnessing RNNs for Financial Time Series Analysis: A Python Approach. Medium. https://medium.com/@redeaddiscolll/harnessing-rnns-for-financial-time-series-analysis-a-python-approach-0669b3a25c7a.   \n",
    "Contribution: EarlyStopping function.\n",
    "\n",
    "#### Libraries\n",
    "- Package Pandas (2.2). (2024). [Python]. https://pandas.pydata.org/\n",
    "- Package NumPy (1.23). (2023). [Pyhton]. https://numpy.org/ - Harris, C. R., Millman, K. J., Van Der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., Van Kerkwijk, M. H., Brett, M., Haldane, A., Del Río, J. F., Wiebe, M., Peterson, P., … Oliphant, T. E. (2020). Array programming with NumPy. Nature, 585(7825), 357–362. https://doi.org/10.1038/s41586-020-2649-2\n",
    "- Droettboom, J. D. H., Michael. (2024). Package matplotlib (3.8.4) [Python]. https://matplotlib.org\n",
    "- Package scikit-learn (1.4). (2024). [Pyhton]. https://scikit-learn.org/stable/index.html\n",
    "- Package Tensorflow (2.16). (2024). [Python]. https://github.com/tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\augus\\Meu Drive\\Data_Projects\\MDS\\master_thesis\\fiscal-balance-forecast\\.venv2\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 7 variables whereas the saved optimizer has 2 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REading File: ../data/data_orig_parameters.csv\n",
      "Outlier Threshold: nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling SimpleRNNCell.call().\n\n\u001b[1mDimensions must be equal, but are 70 and 40 for '{{node sequential_3240_1/simple_rnn_6480_1/simple_rnn_cell_1/MatMul}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](sequential_3240_1/simple_rnn_6480_1/strided_slice_2, sequential_3240_1/simple_rnn_6480_1/simple_rnn_cell_1/Cast/ReadVariableOp)' with input shapes: [?,70], [40,400].\u001b[0m\n\nArguments received by SimpleRNNCell.call():\n  • sequence=tf.Tensor(shape=(None, 70), dtype=float32)\n  • states=('tf.Tensor(shape=(None, 400), dtype=float32)',)\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 124\u001b[0m\n\u001b[0;32m    120\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39mpatience, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# Stoppatience=patience, restore_best_weights training when the validation loss is no longer decreasing after X epochs\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# Let's predict the test set using the best model\u001b[39;00m\n\u001b[0;32m    131\u001b[0m predictions_test_scaled \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\augus\\Meu Drive\\Data_Projects\\MDS\\master_thesis\\fiscal-balance-forecast\\.venv2\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\augus\\Meu Drive\\Data_Projects\\MDS\\master_thesis\\fiscal-balance-forecast\\.venv2\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling SimpleRNNCell.call().\n\n\u001b[1mDimensions must be equal, but are 70 and 40 for '{{node sequential_3240_1/simple_rnn_6480_1/simple_rnn_cell_1/MatMul}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](sequential_3240_1/simple_rnn_6480_1/strided_slice_2, sequential_3240_1/simple_rnn_6480_1/simple_rnn_cell_1/Cast/ReadVariableOp)' with input shapes: [?,70], [40,400].\u001b[0m\n\nArguments received by SimpleRNNCell.call():\n  • sequence=tf.Tensor(shape=(None, 70), dtype=float32)\n  • states=('tf.Tensor(shape=(None, 400), dtype=float32)',)\n  • training=True"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import random as python_random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (mean_absolute_error,\n",
    "                             mean_absolute_percentage_error,\n",
    "                             mean_squared_error)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "import useful_functions as uf\n",
    "\n",
    "# Possible datasets to test - Adjust according to the dataset used to train the desired model, which will be loaded later\n",
    "file_paths = [\n",
    "    #'../data/data_orig_parameters.csv',\n",
    "    #'../data/data_cleaned_RF.csv',\n",
    "    '../data/data_cleaned_LASSO.csv',\n",
    "    #'../data/data_cleaned_RFE.csv'\n",
    "]\n",
    "\n",
    "# Store the GDP data for later use\n",
    "feature_gdp = pd.read_csv('../data/data_orig_parameters.csv', \n",
    "                          parse_dates=['Date'], \n",
    "                          index_col='Date')[['ECO_GDP_R$_12_months']]\n",
    "\n",
    "# List of outlier removal thresholds to test\n",
    "outlier_thresholds = [np.nan, 0.05, 0.10, 0.15, 0.20]\n",
    "#outlier_thresholds = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09]\n",
    "\n",
    "# Dictionary to store the errors\n",
    "errors_dict = {}\n",
    "\n",
    "# Load the model from the file\n",
    "best_model = load_model('models_parameters/RNN_LASSO_BS_128_EP_20_PT_10.keras')\n",
    "# Adjust these parameters mannually\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "patience = 10\n",
    "\n",
    "# Let´s define the seed for reproducibility\n",
    "def func_set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    python_random.seed(seed)\n",
    "    set_seed(seed) #tensorflow.random.set_seed(seed)\n",
    "# Call the function to set the seed\n",
    "func_set_seed(42)\n",
    "\n",
    "# Loop through the files and outlier thresholds\n",
    "for file_path in file_paths:\n",
    "    print(f\"REading File: {file_path}\")\n",
    "    for remove_outliers_threshold in outlier_thresholds:\n",
    "        print(f\"Outlier Threshold: {remove_outliers_threshold}\")\n",
    "        # Load  data\n",
    "        df_raw = pd.read_csv(file_path, parse_dates=['Date'], index_col='Date')\n",
    "        target_variable = df_raw.columns[0]\n",
    "        \n",
    "        # Convert all columns to float\n",
    "        df_raw = df_raw.astype('float64')   \n",
    "        df = df_raw.copy()\n",
    "\n",
    "        # Remove outliers using the threshold\n",
    "        if not pd.isna(remove_outliers_threshold): # If the threshold is not NaN\n",
    "            df_cleaned = uf.remove_outliers(df.copy(), threshold=remove_outliers_threshold)\n",
    "        else: # If the threshold is NaN, don't remove any outliers\n",
    "            df_cleaned = df.copy()\n",
    "\n",
    "        # After removing the outliers, we need to fill the missing values\n",
    "        df_adjusted = uf.fill_missing_values(df_cleaned)\n",
    "\n",
    "        # Define test, train and validation set sizes\n",
    "        val_size = 48 # 48 months or 4 years\n",
    "        test_size = 48 # 48 months or 4 years\n",
    "\n",
    "        # Split the data into train and test sets\n",
    "        train_raw_total = df_adjusted[:-test_size] # This total trainning set will be used to train the final model\n",
    "        df_train = train_raw_total[:-val_size]\n",
    "        df_val = train_raw_total[-val_size:]\n",
    "        df_test = df_adjusted[-test_size:]\n",
    "\n",
    "        # Let´s scale the dfs\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        scaled_train = scaler.fit_transform(df_train)\n",
    "        scaled_val = scaler.transform(df_val)\n",
    "        scaled_test = scaler.transform(df_test)\n",
    "        # include df columns names in the train and test sets\n",
    "        train = pd.DataFrame(scaled_train, columns=df_train.columns)\n",
    "        val = pd.DataFrame(scaled_val, columns=df_val.columns)\n",
    "        test = pd.DataFrame(scaled_test, columns=df_test.columns)\n",
    "        # Include the index in the train and test sets\n",
    "        train.index = df_train.index\n",
    "        val.index = df_val.index\n",
    "        test.index = df_test.index\n",
    "\n",
    "        # Converting the series to samples\n",
    "        # We will use the past 12 months to predict a month 12 months in the future\n",
    "        def createXY(dataset, n_past, n_future):\n",
    "            dataX, dataY = [], []\n",
    "            # Loop for the entire dataset\n",
    "            for i in range(n_past, len(dataset) - n_future + 1):\n",
    "                dataX.append(dataset.iloc[i - n_past:i].values)  # Past n months\n",
    "                dataY.append(dataset.iloc[i + n_future - 1, 0])  #\n",
    "            return np.array(dataX), np.array(dataY)\n",
    "\n",
    "        n_past = 12  # Number of past months to use\n",
    "        n_future = 12  # Number of future months to predict\n",
    "\n",
    "        # Create the samples\n",
    "        X_train, Y_train = createXY(train, n_past, n_future)\n",
    "        X_val, Y_val = createXY(val, n_past, n_future)\n",
    "        X_test, Y_test = createXY(test, n_past, n_future)\n",
    "\n",
    "        # Define EarlyStopping callback\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True) # Stoppatience=patience, restore_best_weights training when the validation loss is no longer decreasing after X epochs\n",
    "\n",
    "\n",
    "        # train the model\n",
    "        history = best_model.fit(X_train, Y_train, \n",
    "                                 validation_data=(X_val, Y_val), \n",
    "                                 epochs=epochs, batch_size=batch_size, \n",
    "                                 verbose=0,\n",
    "                                 callbacks=[early_stopping])\n",
    "        \n",
    "        # Let's predict the test set using the best model\n",
    "        predictions_test_scaled = best_model.predict(X_test)\n",
    "\n",
    "        # Let's reshape the predictions and Y_val to revert the scaling\n",
    "        # Reshape predictions to 2D\n",
    "        predictions_test_scaled_2d = predictions_test_scaled.reshape(-1, 1)\n",
    "        \n",
    "        # Get the last timestep of X_test\n",
    "        X_test_last_timestep = X_test[:, -1, :]\n",
    "        \n",
    "        # Replace the first column of X_test_last_timestep with the scaled predictions.\n",
    "        X_test_last_timestep[:, 0] = predictions_test_scaled_2d[:, 0]\n",
    "        \n",
    "        # unscale the predictions\n",
    "        predictions_test_rescaled = scaler.inverse_transform(X_test_last_timestep)[:, 0]\n",
    "\n",
    "        # Let's convert the predictions and Y_test to a dataframe usind the index from test\n",
    "        predictions_test_df = pd.DataFrame(predictions_test_rescaled, index=test.index[-len(predictions_test_rescaled):], columns=[target_variable])\n",
    "        predictions = predictions_test_df.copy()\n",
    "        \n",
    "        # Get the original Y_test values to calculate the error\n",
    "        Y_test = df_adjusted[-len(predictions):][[target_variable]]\n",
    "\n",
    "        # Convert the predictions and test set to BRL million to calculate the errors\n",
    "        predictions_BRL = uf.convert_pct_GDP_to_BRL(predictions, feature_gdp)\n",
    "        df_test_BRL = uf.convert_pct_GDP_to_BRL(Y_test, feature_gdp)\n",
    "\n",
    "        # Calculate the error\n",
    "        mape_best_GRU = mean_absolute_percentage_error(df_test_BRL, predictions_BRL).round(2)\n",
    "        rmse_best_GRU = np.sqrt(mean_squared_error(df_test_BRL, predictions_BRL)).round(2)\n",
    "        mae_best_GRU = mean_absolute_error(df_test_BRL, predictions_BRL).round(2)\n",
    "\n",
    "        # Store the errors in the dictionary\n",
    "        errors_dict[(file_path, remove_outliers_threshold)] = {'MAPE': mape_best_GRU, 'RMSE': rmse_best_GRU, 'MAE': mae_best_GRU}\n",
    "\n",
    "# Print the errors\n",
    "for key, value in errors_dict.items():\n",
    "    print(f\"File: {key[0]}, Outlier Threshold: {key[1]} -> Errors: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
