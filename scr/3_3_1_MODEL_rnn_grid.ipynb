{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27e86681",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27e86681",
    "outputId": "1af8a1db-879d-44d0-b1df-07a82eaa1fb8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# #Let's Examine Correlation between the series\n",
    "# ###https://github.com/mikekeith52/scalecast-examples/blob/main/multivariate/multivariate.ipynb\n",
    "\n",
    "import useful_functions as uf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.regularizers import l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cs469n2XOr_-",
   "metadata": {
    "id": "cs469n2XOr_-"
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "#file_path = '../data/data_orig_parameters.csv'\n",
    "#file_path = '../data/BR_param_EDA.csv'\n",
    "#file_path = '../data/data_cleaned_RF.csv'\n",
    "#file_path = '../data/data_cleaned_LASSO.csv'\n",
    "file_path = '../data/data_cleaned_RFE.csv'\n",
    "# parse the date column and set it as the index of the dataframe\n",
    "df_raw = pd.read_csv(file_path, parse_dates=['Date'], index_col='Date')\n",
    "# define the target variable as the first column\n",
    "target_variable = df_raw.columns[0]\n",
    "# Convert all columns to float\n",
    "df_raw = df_raw.astype('float64')\n",
    "\n",
    "# Remove outliers\n",
    "remove_outliers_threshold = np.nan\n",
    "#remove_outliers_threshold = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e06252c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any NA value\n",
    "#print(df_raw.isna().sum())\n",
    "\n",
    "# If we want to remove outliers\n",
    "if not pd.isna(remove_outliers_threshold):\n",
    "    df_cleaned = uf.remove_outliers(df_raw.copy(), threshold=remove_outliers_threshold)\n",
    "else:\n",
    "    df_cleaned = df_raw.copy()\n",
    "\n",
    "# Fill missing values\n",
    "df_adjusted = uf.fill_missing_values(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c4bc94dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any collum with unique values and drop it\n",
    "for column in df_adjusted.columns:\n",
    "    if len(df_adjusted[column].unique()) == 1:\n",
    "        df_adjusted.drop(column, axis=1, inplace=True)\n",
    "\n",
    "#print(df_adjusted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b5ef4449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test and validation set sizes\n",
    "val_size = 48 # 48 months or 4 years\n",
    "test_size = 48 # 48 months or 4 years\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_raw_total = df_adjusted.copy()[:-test_size] # This total trainning set will be used to train the final model\n",
    "train_raw = train_raw_total[:-val_size]\n",
    "val_raw = train_raw_total[-val_size:]\n",
    "test_raw = df_adjusted.copy()[-test_size:]\n",
    "\n",
    "# # FIll missing values\n",
    "df_train = uf.fill_missing_values(train_raw)\n",
    "df_val = uf.fill_missing_values(val_raw)\n",
    "df_test = uf.fill_missing_values(test_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "db93b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LetÂ´s scale the dfs\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_train = scaler.fit_transform(df_train)\n",
    "scaled_val = scaler.transform(df_val)\n",
    "scaled_test = scaler.transform(df_test)\n",
    "# include df columns names in the train and test sets\n",
    "train = pd.DataFrame(scaled_train, columns=df_train.columns)\n",
    "val = pd.DataFrame(scaled_val, columns=df_val.columns)\n",
    "test = pd.DataFrame(scaled_test, columns=df_test.columns)\n",
    "# Include the index in the train and test sets\n",
    "train.index = df_train.index\n",
    "val.index = df_val.index\n",
    "test.index = df_test.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f39293",
   "metadata": {},
   "source": [
    "Reshape your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e25733e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the series to samples\n",
    "# We will use the past 12 months to predict the next 12 months\n",
    "def createXY(dataset, n_past, n_future):\n",
    "    dataX, dataY = [], []\n",
    "    # Loop for the entire dataset\n",
    "    for i in range(n_past, len(dataset) - n_future + 1):\n",
    "        dataX.append(dataset.iloc[i - n_past:i].values)  # Past n months\n",
    "        dataY.append(dataset.iloc[i + n_future - 1, 0])  #\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "n_past = 12  # Number of past months to use\n",
    "n_future = 12  # Number of future months to predict\n",
    "\n",
    "# Create the samples\n",
    "X_train, Y_train = createXY(train, n_past, n_future)\n",
    "X_val, Y_val = createXY(val, n_past, n_future)\n",
    "X_test, Y_test = createXY(test, n_past, n_future)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3f8c6",
   "metadata": {},
   "source": [
    "### Let's grid to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "15a5db0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  2 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  3 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  4 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  5 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  6 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  7 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  8 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  9 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  10 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  11 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  12 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  13 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  14 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  15 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  16 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  17 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  18 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  19 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  20 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  21 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  22 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  23 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  24 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  25 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  26 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  27 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  28 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  29 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  30 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  31 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  32 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  33 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  34 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  35 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  36 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  37 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  38 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  39 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  40 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  41 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  42 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  43 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  44 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  45 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  46 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  47 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  48 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  49 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  50 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  51 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  52 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  53 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  54 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  55 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  56 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  57 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  58 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  59 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  60 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  61 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  62 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  63 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  64 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  65 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  66 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  67 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  68 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  69 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  70 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  71 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  72 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  73 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  74 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  75 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  76 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  77 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  78 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  79 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  80 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  81 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  82 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  83 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  84 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  85 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  86 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  87 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  88 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  89 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  90 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  91 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  92 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  93 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  94 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  95 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  96 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  97 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  98 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  99 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  100 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  101 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  102 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  103 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  104 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  105 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  106 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  107 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  108 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  109 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  110 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  111 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  112 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  113 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  114 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  115 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  116 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  117 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  118 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  119 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  120 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  121 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  122 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  123 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  124 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  125 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  126 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  127 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  128 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  129 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  130 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  131 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  132 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  133 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  134 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  135 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  136 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  137 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  138 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  139 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  140 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  141 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  142 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  143 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  144 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  145 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  146 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  147 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  148 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  149 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  150 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  151 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  152 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  153 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  154 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  155 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  156 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  157 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  158 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  159 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  160 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  161 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  162 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  163 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  164 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  165 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  166 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  167 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  168 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  169 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  170 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  171 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  172 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  173 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  174 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  175 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  176 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  177 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  178 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  179 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  180 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  181 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  182 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  183 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  184 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  185 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  186 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  187 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  188 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  189 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  190 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  191 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  192 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  193 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  194 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  195 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  196 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  197 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  198 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  199 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  200 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  201 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  202 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  203 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  204 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  205 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  206 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  207 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  208 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  209 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  210 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  211 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  212 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  213 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  214 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  215 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  216 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  217 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  218 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  219 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  220 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  221 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  222 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  223 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  224 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  225 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  226 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  227 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  228 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  229 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  230 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  231 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  232 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  233 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  234 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  235 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  236 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  237 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  238 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  239 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  240 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  241 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  242 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  243 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  244 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  245 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  246 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  247 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  248 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  249 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  250 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  251 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  252 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  253 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  254 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  255 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  256 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  257 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  258 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  259 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  260 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  261 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  262 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  263 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  264 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  265 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  266 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  267 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  268 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  269 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  270 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  271 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  272 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  273 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  274 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  275 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  276 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  277 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  278 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  279 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  280 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  281 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  282 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  283 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  284 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  285 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  286 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  287 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  288 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  289 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  290 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  291 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  292 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  293 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  294 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  295 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  296 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  297 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  298 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  299 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  300 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  301 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  302 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  303 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  304 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  305 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  306 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  307 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  308 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  309 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  310 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  311 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  312 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  313 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  314 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  315 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  316 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  317 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  318 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  319 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  320 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  321 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  322 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  323 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  324 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  325 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  326 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  327 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  328 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  329 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  330 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  331 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  332 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  333 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  334 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  335 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  336 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  337 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  338 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  339 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  340 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  341 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  342 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  343 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  344 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  345 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  346 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  347 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  348 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  349 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  350 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  351 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  352 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  353 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  354 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  355 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  356 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  357 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  358 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  359 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  360 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  361 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  362 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  363 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  364 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  365 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  366 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  367 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  368 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  369 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  370 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  371 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  372 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  373 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  374 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  375 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  376 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  377 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  378 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  379 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  380 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  381 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  382 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  383 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  384 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  385 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  386 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  387 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  388 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  389 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  390 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  391 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  392 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  393 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  394 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  395 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  396 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  397 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  398 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  399 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  400 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  401 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  402 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  403 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  404 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  405 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  406 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  407 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  408 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  409 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  410 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  411 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  412 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  413 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  414 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  415 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  416 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  417 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  418 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  419 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  420 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  421 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  422 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  423 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  424 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  425 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  426 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  427 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  428 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  429 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  430 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  431 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  432 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  433 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  434 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  435 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  436 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  437 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  438 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  439 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  440 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  441 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  442 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  443 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  444 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  445 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  446 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  447 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  448 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  449 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  450 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  451 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  452 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  453 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  454 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  455 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  456 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  457 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  458 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  459 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  460 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  461 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  462 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  463 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  464 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  465 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  466 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  467 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  468 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  469 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  470 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  471 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  472 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  473 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  474 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  475 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  476 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  477 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  478 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  479 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  480 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  481 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  482 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  483 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  484 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  485 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  486 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  487 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  488 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  489 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  490 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  491 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  492 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  493 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  494 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  495 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  496 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  497 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  498 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  499 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  500 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  501 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  502 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  503 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  504 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  505 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  506 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  507 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  508 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  509 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  510 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  511 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  512 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  513 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  514 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  515 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  516 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  517 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  518 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  519 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  520 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  521 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  522 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  523 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  524 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  525 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  526 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  527 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  528 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  529 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  530 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  531 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  532 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  533 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  534 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  535 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  536 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  537 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  538 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  539 th iteration\n",
      "Applying parameters:  {'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  540 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  541 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  542 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  543 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  544 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  545 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  546 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  547 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  548 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  549 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  550 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  551 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  552 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  553 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  554 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  555 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  556 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  557 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  558 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  559 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  560 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  561 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  562 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  563 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  564 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  565 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  566 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  567 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  568 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  569 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  570 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  571 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  572 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  573 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  574 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  575 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  576 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  577 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  578 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  579 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  580 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  581 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  582 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  583 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  584 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  585 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  586 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  587 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  588 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  589 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  590 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  591 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  592 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  593 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  594 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  595 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  596 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  597 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  598 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  599 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  600 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  601 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  602 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  603 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  604 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  605 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  606 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  607 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  608 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  609 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  610 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  611 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  612 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  613 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  614 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  615 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  616 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  617 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  618 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  619 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  620 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  621 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  622 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  623 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  624 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  625 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  626 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  627 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  628 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  629 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  630 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  631 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  632 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  633 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  634 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  635 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  636 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  637 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  638 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  639 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  640 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  641 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  642 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  643 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  644 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  645 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  646 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  647 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  648 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  649 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  650 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  651 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  652 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  653 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  654 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  655 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  656 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  657 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  658 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  659 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  660 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  661 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  662 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  663 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  664 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  665 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  666 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  667 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  668 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  669 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  670 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  671 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  672 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  673 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  674 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  675 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  676 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  677 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  678 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  679 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  680 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  681 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  682 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  683 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  684 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  685 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  686 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  687 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  688 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  689 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  690 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  691 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  692 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  693 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  694 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  695 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  696 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  697 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  698 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  699 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  700 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  701 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  702 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  703 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  704 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  705 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  706 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  707 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  708 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  709 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  710 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  711 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  712 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  713 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  714 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  715 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  716 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  717 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  718 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  719 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  720 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  721 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  722 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  723 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  724 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  725 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  726 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  727 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  728 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  729 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  730 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  731 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  732 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  733 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  734 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  735 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  736 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  737 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  738 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  739 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  740 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  741 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  742 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  743 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  744 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  745 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  746 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  747 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  748 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  749 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  750 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  751 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  752 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  753 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  754 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  755 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  756 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  757 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  758 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  759 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  760 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  761 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  762 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  763 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  764 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  765 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  766 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  767 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  768 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  769 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  770 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  771 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  772 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  773 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  774 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  775 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  776 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  777 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  778 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  779 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  780 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  781 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  782 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  783 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  784 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  785 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  786 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  787 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  788 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  789 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  790 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  791 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  792 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  793 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  794 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  795 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  796 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  797 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  798 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  799 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  800 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  801 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  802 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  803 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  804 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  805 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  806 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  807 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  808 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  809 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  810 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  811 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  812 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  813 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  814 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  815 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  816 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  817 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  818 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  819 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  820 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  821 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  822 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  823 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  824 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  825 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  826 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  827 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  828 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  829 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  830 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  831 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  832 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  833 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  834 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  835 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  836 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  837 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  838 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  839 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  840 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  841 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  842 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  843 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  844 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  845 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  846 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  847 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  848 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  849 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  850 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  851 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  852 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  853 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  854 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  855 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  856 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  857 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  858 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  859 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  860 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  861 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  862 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  863 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  864 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  865 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  866 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  867 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  868 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  869 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  870 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  871 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  872 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  873 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  874 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  875 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  876 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  877 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  878 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  879 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  880 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  881 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  882 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  883 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  884 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  885 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  886 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  887 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  888 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  889 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  890 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  891 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  892 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  893 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  894 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  895 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  896 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  897 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  898 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  899 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  900 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  901 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  902 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  903 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  904 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  905 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  906 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  907 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  908 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  909 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  910 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  911 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  912 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  913 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  914 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  915 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  916 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  917 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  918 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  919 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  920 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  921 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  922 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  923 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  924 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  925 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  926 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  927 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  928 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  929 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  930 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  931 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  932 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  933 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  934 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  935 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  936 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  937 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  938 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  939 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  940 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  941 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  942 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  943 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  944 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  945 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  946 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  947 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  948 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  949 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  950 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  951 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  952 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  953 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  954 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  955 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  956 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  957 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  958 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  959 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  960 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  961 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  962 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  963 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  964 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  965 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  966 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  967 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  968 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  969 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  970 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  971 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  972 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  973 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  974 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  975 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  976 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  977 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  978 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  979 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  980 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  981 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  982 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  983 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  984 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  985 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  986 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  987 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  988 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  989 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  990 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  991 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  992 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  993 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  994 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  995 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  996 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  997 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  998 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  999 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1000 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1001 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1002 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1003 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1004 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1005 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1006 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1007 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1008 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1009 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1010 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1011 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1012 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1013 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1014 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1015 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1016 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1017 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1018 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1019 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1020 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1021 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1022 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1023 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1024 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1025 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1026 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1027 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1028 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1029 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1030 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1031 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1032 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1033 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1034 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1035 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1036 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1037 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1038 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1039 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1040 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1041 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1042 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1043 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1044 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1045 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1046 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1047 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1048 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1049 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1050 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1051 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1052 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1053 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1054 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1055 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1056 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1057 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1058 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1059 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1060 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1061 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1062 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1063 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1064 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1065 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1066 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1067 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1068 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1069 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1070 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1071 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1072 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1073 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1074 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1075 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1076 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1077 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1078 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1079 th iteration\n",
      "Applying parameters:  {'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1080 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1081 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1082 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1083 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1084 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1085 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1086 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1087 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1088 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1089 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1090 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1091 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1092 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1093 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1094 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1095 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1096 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1097 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1098 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1099 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1100 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1101 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1102 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1103 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1104 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1105 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1106 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1107 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1108 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1109 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1110 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1111 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1112 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1113 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1114 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1115 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1116 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1117 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1118 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1119 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1120 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1121 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1122 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1123 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1124 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1125 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1126 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1127 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1128 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1129 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1130 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1131 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1132 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1133 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1134 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1135 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1136 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1137 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1138 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1139 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1140 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1141 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1142 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1143 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1144 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1145 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1146 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1147 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1148 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1149 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1150 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1151 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1152 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1153 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1154 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1155 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1156 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1157 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1158 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1159 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1160 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1161 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1162 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1163 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1164 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1165 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1166 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1167 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1168 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1169 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1170 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1171 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1172 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1173 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1174 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1175 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1176 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1177 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1178 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1179 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1180 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1181 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1182 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1183 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1184 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1185 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1186 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1187 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1188 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1189 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1190 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1191 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1192 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1193 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1194 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1195 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1196 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1197 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1198 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1199 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1200 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1201 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1202 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1203 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1204 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1205 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1206 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1207 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1208 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1209 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1210 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1211 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1212 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1213 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1214 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1215 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1216 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1217 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1218 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1219 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1220 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1221 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1222 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1223 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1224 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1225 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1226 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1227 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1228 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1229 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1230 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1231 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1232 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1233 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1234 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1235 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1236 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1237 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1238 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1239 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1240 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1241 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1242 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1243 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1244 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1245 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1246 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1247 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1248 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1249 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1250 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1251 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1252 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1253 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1254 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1255 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1256 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1257 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1258 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1259 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1260 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1261 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1262 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1263 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1264 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1265 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1266 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1267 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1268 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1269 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1270 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1271 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1272 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1273 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1274 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1275 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1276 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1277 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1278 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1279 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1280 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1281 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1282 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1283 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1284 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1285 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1286 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1287 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1288 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1289 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1290 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1291 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1292 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1293 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1294 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1295 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1296 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1297 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1298 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1299 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1300 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1301 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1302 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1303 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1304 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1305 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1306 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1307 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1308 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1309 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1310 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1311 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1312 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1313 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1314 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1315 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1316 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1317 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1318 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1319 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1320 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1321 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1322 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1323 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1324 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1325 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1326 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1327 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1328 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1329 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1330 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1331 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1332 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1333 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1334 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1335 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1336 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1337 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1338 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1339 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1340 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1341 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1342 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1343 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1344 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1345 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1346 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1347 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1348 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1349 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1350 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1351 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1352 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1353 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1354 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1355 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1356 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1357 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1358 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1359 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1360 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1361 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1362 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1363 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1364 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1365 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1366 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1367 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1368 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1369 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1370 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1371 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1372 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1373 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1374 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1375 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1376 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1377 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1378 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1379 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1380 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1381 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1382 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1383 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1384 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1385 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1386 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1387 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1388 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1389 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1390 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1391 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1392 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1393 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1394 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1395 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1396 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1397 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1398 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1399 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1400 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1401 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1402 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1403 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1404 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1405 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1406 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1407 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1408 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1409 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1410 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1411 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1412 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1413 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1414 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1415 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1416 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1417 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1418 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1419 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1420 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1421 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1422 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1423 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1424 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1425 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1426 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1427 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1428 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1429 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1430 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1431 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1432 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1433 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1434 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1435 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1436 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1437 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1438 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1439 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1440 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1441 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1442 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1443 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1444 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1445 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1446 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1447 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1448 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1449 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1450 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1451 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1452 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1453 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1454 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1455 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1456 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1457 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1458 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1459 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1460 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1461 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1462 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1463 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1464 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1465 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1466 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1467 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1468 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1469 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1470 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1471 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1472 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1473 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1474 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1475 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1476 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1477 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1478 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1479 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1480 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1481 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1482 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1483 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1484 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1485 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1486 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1487 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1488 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1489 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1490 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1491 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1492 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1493 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1494 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1495 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1496 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1497 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1498 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1499 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1500 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1501 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1502 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1503 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1504 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1505 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1506 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1507 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1508 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1509 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1510 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1511 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1512 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1513 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1514 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1515 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1516 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1517 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1518 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1519 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1520 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1521 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1522 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1523 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1524 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1525 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1526 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1527 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1528 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1529 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1530 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1531 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1532 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1533 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1534 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1535 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1536 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1537 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1538 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1539 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1540 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1541 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1542 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1543 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1544 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1545 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1546 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1547 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1548 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1549 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1550 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1551 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1552 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1553 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1554 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1555 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1556 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1557 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1558 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1559 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1560 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1561 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1562 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1563 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1564 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1565 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1566 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1567 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1568 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1569 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1570 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1571 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1572 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1573 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1574 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1575 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1576 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1577 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1578 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1579 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1580 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1581 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1582 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1583 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1584 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1585 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1586 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1587 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1588 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1589 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1590 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1591 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1592 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1593 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1594 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1595 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1596 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1597 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1598 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1599 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 1, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1600 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1601 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1602 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1603 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1604 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1605 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1606 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1607 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1608 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1609 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 2, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1610 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1611 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1612 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1613 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1614 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'adam', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1615 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 10, 2: 10, 3: 5}} . This is the  1616 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 50, 2: 30, 3: 15}} . This is the  1617 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 100, 2: 100, 3: 50}} . This is the  1618 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 200, 2: 200, 3: 100}} . This is the  1619 th iteration\n",
      "Applying parameters:  {'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'n_layers': 3, 'optimizer': 'rmsprop', 'rnn_units': {1: 400, 2: 300, 3: 100}} . This is the  1620 th iteration\n",
      "Best Score: 0.09824705123901367\n",
      "Best Parameters: {'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'n_layers': 1, 'optimizer': 'adam', 'rnn_units': {1: 100, 2: 100, 3: 50}}\n"
     ]
    }
   ],
   "source": [
    "### Build a RNN model testing different parameters\n",
    "\n",
    "\n",
    "# Function to build the model\n",
    "def build_model(n_layers = 2, optimizer='adam', learning_rate=0.001, \n",
    "                rnn_units={0: 50,  1: 20, 2: 10},\n",
    "                dropout_rate=0.2, alphas_l1_l2=0.01):\n",
    "    # Check the optimizer\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model = Sequential()\n",
    "    # Define the input layer shape\n",
    "    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    # Add the RNN layers\n",
    "    for i in range(n_layers):\n",
    "        if i < n_layers - 1:  # intermediate hidden layers\n",
    "            model.add(SimpleRNN(units=rnn_units[i+1], return_sequences=True, \n",
    "                                kernel_regularizer=l1_l2(l1=alphas_l1_l2, l2=alphas_l1_l2), \n",
    "                                recurrent_regularizer=l1_l2(l1=alphas_l1_l2, l2=alphas_l1_l2),bias_regularizer=l1_l2(l1=alphas_l1_l2, l2=alphas_l1_l2)))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        else:  # last hidden layer\n",
    "            model.add(SimpleRNN(units=rnn_units[i+1],  \n",
    "                                kernel_regularizer=l1_l2(l1=alphas_l1_l2, l2=alphas_l1_l2), \n",
    "                                recurrent_regularizer=l1_l2(l1=alphas_l1_l2, l2=alphas_l1_l2),bias_regularizer=l1_l2(l1=alphas_l1_l2, l2=alphas_l1_l2)))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(units=1)) #output layer\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Function to test each set of parameters\n",
    "def custom_fit(params):\n",
    "    training_params = {key: params[key] for key in params if key in ['n_layers', 'alphas_l1_l2' \n",
    "                                                                     'dropout_rate',\n",
    "                                                                     'rnn_units', \n",
    "                                                                     'optimizer', \n",
    "                                                                     'learning_rate']} # Use this to pass the parameters to the model\n",
    "    model = build_model(**training_params) # Create the model\n",
    "    \n",
    "    # Define EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # Stop training when the validation loss is no longer decreasing after X epochs\n",
    "\n",
    "    # Fitting the model with early stopping\n",
    "    model.fit(X_train, Y_train, \n",
    "              epochs=params['epochs'], \n",
    "              batch_size=params['batch_size'], \n",
    "              verbose=0,\n",
    "              validation_data=(X_val, Y_val), \n",
    "              callbacks=[early_stopping])\n",
    "    # Compute the loss on the validation set\n",
    "    loss = model.evaluate(X_val, Y_val, verbose=0)\n",
    "    return loss\n",
    "\n",
    "# EspaÃ§o de parÃ¢metros\n",
    "param_grid = {\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'alphas_l1_l2' : [0.001, 0.01, 0.1, 1],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'epochs': [20, 50, 75, 100],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001], # Learning rate\n",
    "    'n_layers': [1,2,3], # Number of hidden layers\n",
    "    'rnn_units' : [{1: 10,  2: 10, 3: 5},\n",
    "                   {1: 50,  2: 30, 3: 15},\n",
    "                   {1: 100,  2: 100, 3: 50}, {1: 200,  2: 200, 3: 100}, {1: 400,  2: 300, 3: 100}]  \n",
    "}\n",
    "\n",
    "# Compare the scores for each set of parameters\n",
    "best_score = np.inf\n",
    "best_params = None\n",
    "interactions = 1\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(\"Applying parameters: \", params,\". This is the \", interactions, \"th iteration\")\n",
    "    interactions += 1\n",
    "    score = custom_fit(params)\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "\n",
    "print(f'Best Score: {best_score}')\n",
    "print(f'Best Parameters: {best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d59d3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's build the model with the best parameters and train it\n",
    "# Get epochs and batch_size from best_params\n",
    "best_params_bkp = best_params.copy()\n",
    "epochs = best_params.pop('epochs')\n",
    "batch_size = best_params.pop('batch_size')\n",
    "\n",
    "# build the model\n",
    "best_model = build_model(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5a90bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "best_model.save('best_rnn_model_grid_rfe_6.keras')  # Saves the model to a HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c1b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we need to restore the best model\n",
    "\n",
    "# # Load the model from the file\n",
    "# best_model = load_model('best_rnn_model_grid.keras')\n",
    "# # from manual hyperparameter tuning\n",
    "# epochs = 20\n",
    "# batch_size = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "544e7993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "5/5 - 4s - 862ms/step - loss: 14.3289 - val_loss: 13.7244\n",
      "Epoch 2/75\n",
      "5/5 - 1s - 165ms/step - loss: 13.7679 - val_loss: 13.6125\n",
      "Epoch 3/75\n",
      "5/5 - 1s - 124ms/step - loss: 13.4839 - val_loss: 13.2094\n",
      "Epoch 4/75\n",
      "5/5 - 0s - 93ms/step - loss: 13.2152 - val_loss: 12.9757\n",
      "Epoch 5/75\n",
      "5/5 - 0s - 93ms/step - loss: 12.8493 - val_loss: 12.6653\n",
      "Epoch 6/75\n",
      "5/5 - 1s - 133ms/step - loss: 12.5445 - val_loss: 12.2975\n",
      "Epoch 7/75\n",
      "5/5 - 1s - 117ms/step - loss: 12.2157 - val_loss: 12.0088\n",
      "Epoch 8/75\n",
      "5/5 - 0s - 91ms/step - loss: 11.8737 - val_loss: 11.7183\n",
      "Epoch 9/75\n",
      "5/5 - 1s - 102ms/step - loss: 11.5362 - val_loss: 11.3042\n",
      "Epoch 10/75\n",
      "5/5 - 1s - 137ms/step - loss: 11.2000 - val_loss: 11.0040\n",
      "Epoch 11/75\n",
      "5/5 - 0s - 99ms/step - loss: 10.8455 - val_loss: 10.6812\n",
      "Epoch 12/75\n",
      "5/5 - 1s - 128ms/step - loss: 10.5114 - val_loss: 10.2900\n",
      "Epoch 13/75\n",
      "5/5 - 0s - 90ms/step - loss: 10.1902 - val_loss: 9.9854\n",
      "Epoch 14/75\n",
      "5/5 - 1s - 133ms/step - loss: 9.8488 - val_loss: 9.6756\n",
      "Epoch 15/75\n",
      "5/5 - 1s - 132ms/step - loss: 9.5109 - val_loss: 9.3014\n",
      "Epoch 16/75\n",
      "5/5 - 1s - 114ms/step - loss: 9.1880 - val_loss: 8.9994\n",
      "Epoch 17/75\n",
      "5/5 - 0s - 94ms/step - loss: 8.8601 - val_loss: 8.6959\n",
      "Epoch 18/75\n",
      "5/5 - 1s - 138ms/step - loss: 8.5501 - val_loss: 8.3642\n",
      "Epoch 19/75\n",
      "5/5 - 0s - 83ms/step - loss: 8.2355 - val_loss: 8.0671\n",
      "Epoch 20/75\n",
      "5/5 - 0s - 91ms/step - loss: 7.9311 - val_loss: 7.7600\n",
      "Epoch 21/75\n",
      "5/5 - 1s - 104ms/step - loss: 7.6439 - val_loss: 7.4646\n",
      "Epoch 22/75\n",
      "5/5 - 1s - 118ms/step - loss: 7.3512 - val_loss: 7.1997\n",
      "Epoch 23/75\n",
      "5/5 - 0s - 98ms/step - loss: 7.0754 - val_loss: 6.9038\n",
      "Epoch 24/75\n",
      "5/5 - 1s - 102ms/step - loss: 6.7917 - val_loss: 6.6429\n",
      "Epoch 25/75\n",
      "5/5 - 1s - 143ms/step - loss: 6.5232 - val_loss: 6.3693\n",
      "Epoch 26/75\n",
      "5/5 - 0s - 88ms/step - loss: 6.2639 - val_loss: 6.1118\n",
      "Epoch 27/75\n",
      "5/5 - 0s - 86ms/step - loss: 6.0147 - val_loss: 5.8585\n",
      "Epoch 28/75\n",
      "5/5 - 1s - 133ms/step - loss: 5.7527 - val_loss: 5.6225\n",
      "Epoch 29/75\n",
      "5/5 - 1s - 127ms/step - loss: 5.5193 - val_loss: 5.3831\n",
      "Epoch 30/75\n",
      "5/5 - 0s - 73ms/step - loss: 5.2806 - val_loss: 5.1536\n",
      "Epoch 31/75\n",
      "5/5 - 0s - 80ms/step - loss: 5.0580 - val_loss: 4.9316\n",
      "Epoch 32/75\n",
      "5/5 - 0s - 77ms/step - loss: 4.8385 - val_loss: 4.7181\n",
      "Epoch 33/75\n",
      "5/5 - 0s - 78ms/step - loss: 4.6280 - val_loss: 4.5136\n",
      "Epoch 34/75\n",
      "5/5 - 0s - 77ms/step - loss: 4.4226 - val_loss: 4.3144\n",
      "Epoch 35/75\n",
      "5/5 - 0s - 80ms/step - loss: 4.2291 - val_loss: 4.1224\n",
      "Epoch 36/75\n",
      "5/5 - 0s - 73ms/step - loss: 4.0375 - val_loss: 3.9393\n",
      "Epoch 37/75\n",
      "5/5 - 1s - 140ms/step - loss: 3.8546 - val_loss: 3.7595\n",
      "Epoch 38/75\n",
      "5/5 - 0s - 74ms/step - loss: 3.6840 - val_loss: 3.5913\n",
      "Epoch 39/75\n",
      "5/5 - 0s - 78ms/step - loss: 3.5121 - val_loss: 3.4234\n",
      "Epoch 40/75\n",
      "5/5 - 1s - 134ms/step - loss: 3.3486 - val_loss: 3.2651\n",
      "Epoch 41/75\n",
      "5/5 - 0s - 92ms/step - loss: 3.1934 - val_loss: 3.1108\n",
      "Epoch 42/75\n",
      "5/5 - 1s - 130ms/step - loss: 3.0392 - val_loss: 2.9668\n",
      "Epoch 43/75\n",
      "5/5 - 1s - 100ms/step - loss: 2.8983 - val_loss: 2.8261\n",
      "Epoch 44/75\n",
      "5/5 - 0s - 82ms/step - loss: 2.7610 - val_loss: 2.6918\n",
      "Epoch 45/75\n",
      "5/5 - 0s - 83ms/step - loss: 2.6276 - val_loss: 2.5643\n",
      "Epoch 46/75\n",
      "5/5 - 0s - 73ms/step - loss: 2.5077 - val_loss: 2.4420\n",
      "Epoch 47/75\n",
      "5/5 - 0s - 98ms/step - loss: 2.3871 - val_loss: 2.3254\n",
      "Epoch 48/75\n",
      "5/5 - 0s - 76ms/step - loss: 2.2710 - val_loss: 2.2142\n",
      "Epoch 49/75\n",
      "5/5 - 1s - 154ms/step - loss: 2.1614 - val_loss: 2.1075\n",
      "Epoch 50/75\n",
      "5/5 - 1s - 120ms/step - loss: 2.0561 - val_loss: 2.0066\n",
      "Epoch 51/75\n",
      "5/5 - 0s - 80ms/step - loss: 1.9605 - val_loss: 1.9089\n",
      "Epoch 52/75\n",
      "5/5 - 0s - 78ms/step - loss: 1.8627 - val_loss: 1.8166\n",
      "Epoch 53/75\n",
      "5/5 - 1s - 132ms/step - loss: 1.7716 - val_loss: 1.7292\n",
      "Epoch 54/75\n",
      "5/5 - 1s - 132ms/step - loss: 1.6880 - val_loss: 1.6460\n",
      "Epoch 55/75\n",
      "5/5 - 0s - 84ms/step - loss: 1.6045 - val_loss: 1.5672\n",
      "Epoch 56/75\n",
      "5/5 - 0s - 80ms/step - loss: 1.5270 - val_loss: 1.4920\n",
      "Epoch 57/75\n",
      "5/5 - 0s - 88ms/step - loss: 1.4547 - val_loss: 1.4209\n",
      "Epoch 58/75\n",
      "5/5 - 0s - 75ms/step - loss: 1.3848 - val_loss: 1.3534\n",
      "Epoch 59/75\n",
      "5/5 - 0s - 72ms/step - loss: 1.3160 - val_loss: 1.2886\n",
      "Epoch 60/75\n",
      "5/5 - 0s - 70ms/step - loss: 1.2561 - val_loss: 1.2271\n",
      "Epoch 61/75\n",
      "5/5 - 1s - 133ms/step - loss: 1.1915 - val_loss: 1.1689\n",
      "Epoch 62/75\n",
      "5/5 - 0s - 72ms/step - loss: 1.1361 - val_loss: 1.1127\n",
      "Epoch 63/75\n",
      "5/5 - 0s - 71ms/step - loss: 1.0811 - val_loss: 1.0596\n",
      "Epoch 64/75\n",
      "5/5 - 0s - 77ms/step - loss: 1.0265 - val_loss: 1.0079\n",
      "Epoch 65/75\n",
      "5/5 - 0s - 80ms/step - loss: 0.9793 - val_loss: 0.9594\n",
      "Epoch 66/75\n",
      "5/5 - 0s - 88ms/step - loss: 0.9319 - val_loss: 0.9136\n",
      "Epoch 67/75\n",
      "5/5 - 0s - 88ms/step - loss: 0.8892 - val_loss: 0.8703\n",
      "Epoch 68/75\n",
      "5/5 - 0s - 77ms/step - loss: 0.8446 - val_loss: 0.8285\n",
      "Epoch 69/75\n",
      "5/5 - 0s - 98ms/step - loss: 0.8025 - val_loss: 0.7891\n",
      "Epoch 70/75\n",
      "5/5 - 0s - 90ms/step - loss: 0.7635 - val_loss: 0.7515\n",
      "Epoch 71/75\n",
      "5/5 - 0s - 99ms/step - loss: 0.7264 - val_loss: 0.7159\n",
      "Epoch 72/75\n",
      "5/5 - 1s - 120ms/step - loss: 0.6916 - val_loss: 0.6825\n",
      "Epoch 73/75\n",
      "5/5 - 0s - 87ms/step - loss: 0.6573 - val_loss: 0.6502\n",
      "Epoch 74/75\n",
      "5/5 - 0s - 87ms/step - loss: 0.6249 - val_loss: 0.6219\n",
      "Epoch 75/75\n",
      "5/5 - 1s - 134ms/step - loss: 0.5983 - val_loss: 0.5916\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# train the model\n",
    "history = best_model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "# Make predictions\n",
    "predictions_scaled = best_model.predict(X_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62bd6363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "016ae6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE RNN: 1.5760232210159302\n",
      "RMSE RNN: 18820.96484375\n",
      "MAE RNN: 12424.9765625\n"
     ]
    }
   ],
   "source": [
    "# Let's reshape the predictions and Y_val to revert the scaling\n",
    "# Reshape predictions to 2D\n",
    "predictions_scaled_2d = predictions_scaled.reshape(-1, 1)\n",
    "# Get the last timestep of X_val\n",
    "X_val_last_timestep = X_val[:, -1, :]\n",
    "# Replace the first column of X_val_last_timestep with the scaled predictions.\n",
    "X_val_last_timestep[:, 0] = predictions_scaled_2d[:, 0]\n",
    "# unscale the predictions\n",
    "predictions_rescaled = scaler.inverse_transform(X_val_last_timestep)[:, 0]\n",
    "\n",
    "# unscale the Y_val\n",
    "#Y_val_2d = Y_val.reshape(-1, 1)\n",
    "#Y_val_rescaled = scaler.inverse_transform(val.iloc[n_past:, :].values)[:, 0]\n",
    "Y_val_rescaled = scaler.inverse_transform(val.iloc[-len(predictions_scaled):, :].values)[:, 0]\n",
    "\n",
    "# Calculate the error\n",
    "mape = mean_absolute_percentage_error(Y_val_rescaled, predictions_rescaled)\n",
    "rmse = np.sqrt(mean_squared_error(Y_val_rescaled, predictions_rescaled))\n",
    "mae = mean_absolute_error(Y_val_rescaled, predictions_rescaled)\n",
    "\n",
    "print(f'MAPE RNN: {mape}')\n",
    "print(f'RMSE RNN: {rmse}')\n",
    "print(f'MAE RNN: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "32e719f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n"
     ]
    }
   ],
   "source": [
    "# Let's predict the test set using the best model\n",
    "predictions_test_scaled = best_model.predict(X_test)\n",
    "# Make predictions\n",
    "#predictions_test_scaled = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a80f71c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reshape the predictions and Y_val to revert the scaling\n",
    "# Reshape predictions to 2D\n",
    "predictions_test_scaled_2d = predictions_test_scaled.reshape(-1, 1)\n",
    "# Get the last timestep of X_test\n",
    "X_test_last_timestep = X_test[:, -1, :]\n",
    "# Replace the first column of X_test_last_timestep with the scaled predictions.\n",
    "X_test_last_timestep[:, 0] = predictions_test_scaled_2d[:, 0]\n",
    "# unscale the predictions\n",
    "predictions_test_rescaled = scaler.inverse_transform(X_test_last_timestep)[:, 0]\n",
    "\n",
    "# Let's convert the predictions and Y_test to a dataframe usind the index from test\n",
    "predictions_test_df = pd.DataFrame(predictions_test_rescaled, index=test.index[-len(predictions_test_rescaled):], columns=[target_variable])\n",
    "\n",
    "# Reverse the decomposition of the time series\n",
    "#predictions = recompose_time_series(predictions_test_df, decomp_dict)\n",
    "predictions = predictions_test_df.copy()\n",
    "Y_test = df_adjusted[-len(predictions):][target_variable]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f5a61945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE best RNN: 1.1584241390228271\n",
      "RMSE best RNN: 43011.71484375\n",
      "MAE best RNN: 33081.02734375\n"
     ]
    }
   ],
   "source": [
    "# Calculate the error\n",
    "mape_best_RNN = mean_absolute_percentage_error(Y_test, predictions)\n",
    "rmse_best_RNN = np.sqrt(mean_squared_error(Y_test, predictions))\n",
    "mae_best_RNN = mean_absolute_error(Y_test, predictions)\n",
    "\n",
    "print(f'MAPE best RNN: {mape_best_RNN}')\n",
    "print(f'RMSE best RNN: {rmse_best_RNN}')\n",
    "print(f'MAE best RNN: {mae_best_RNN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot RNN best Model Forecast(static) vs Actuals\n",
    "\n",
    "# Convert Y_test_rescaled to a dataframe using the test index\n",
    "Y_test_rescaled_df = pd.DataFrame(Y_test, index=Y_test.index)\n",
    "#Y_test_rescaled_df = pd.DataFrame(Y_test, index=test.index[n_past:])\n",
    "# Change the column name to the target variable\n",
    "Y_test_rescaled_df.columns = [target_variable]\n",
    "\n",
    "uf.plot_forecast_vs_test(\n",
    "    target_variable, \n",
    "    #train[target_variable], \n",
    "    Y_test_rescaled_df[target_variable],\n",
    "    predictions,\n",
    "    #predictions_test_rescaled, \n",
    "    'RNN Model Forecast vs Actuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337dee0",
   "metadata": {},
   "source": [
    "Let's forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a419f2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n"
     ]
    }
   ],
   "source": [
    "# Adjust the index to datetime format\n",
    "df_adjusted.index = pd.to_datetime(df_adjusted.index)\n",
    "\n",
    "# New list to store the forecasts\n",
    "monthly_forecasts = []\n",
    "\n",
    "# Define a start date for the forecast\n",
    "start_date = pd.Timestamp(year=2024, month=1, day=1)\n",
    "\n",
    "for i in range(12):  # For each month in the next year\n",
    "    # Define a end date for the forecast\n",
    "    end_date = start_date - pd.DateOffset(months=13)\n",
    "    # Define a start date 12 months before the end date\n",
    "    start_date_12_months_before = end_date - pd.DateOffset(months=11)\n",
    "    # Subset the dataset to get the data for the last 12 months\n",
    "    data_for_prediction = df_adjusted.loc[start_date_12_months_before:end_date]\n",
    "    # Scale the data\n",
    "    data_for_prediction_scaled = scaler.transform(data_for_prediction)\n",
    "    data_for_prediction_scaled_df = pd.DataFrame(data_for_prediction_scaled, columns=df_adjusted.columns) # convert to dataframe\n",
    "    # Include the index in the train and test sets\n",
    "    data_for_prediction_scaled_df.index = data_for_prediction.index\n",
    "    \n",
    "    # Prepare the data for the model\n",
    "    X_predict = np.array([data_for_prediction_scaled_df]) # convert to numpy array\n",
    "    # Predict the next month\n",
    "    future_prediction_scaled = best_model.predict(X_predict)\n",
    "    \n",
    "    # Reverse the scaling\n",
    "    temp_array = np.zeros((1, len(df_adjusted.columns))) # Create an array of zeros\n",
    "    temp_array[0, 0] = future_prediction_scaled[0, 0] # Store the prediction in the first column\n",
    "\n",
    "    # rescale the prediction\n",
    "    future_prediction_rescaled = scaler.inverse_transform(temp_array)[0, 0]\n",
    "\n",
    "    ### Compute confidence intervals\n",
    "    # Compute squared errors for each prediction point\n",
    "    predictions_array = predictions.values.flatten() # Convert the predictions to a 1D array\n",
    "    squared_errors = (Y_test - predictions_array) ** 2\n",
    "    rmse_errors = np.sqrt(squared_errors) # Take the square root of each squared error to get the RMSE for each point\n",
    "    # Compute the standard error\n",
    "    error_std = np.std(rmse_errors, axis=0)\n",
    "    # Apply it to your forecasts\n",
    "    lower_bound = future_prediction_rescaled - 1.96 * error_std # 95% confidence interval\n",
    "    upper_bound = future_prediction_rescaled + 1.96 * error_std\n",
    "\n",
    "\n",
    "    # Append the prediction to the list\n",
    "    #monthly_forecasts.append(future_prediction_rescaled)\n",
    "    # Store the prediction and bounds\n",
    "    monthly_forecasts.append((future_prediction_rescaled, lower_bound, upper_bound))\n",
    "    # Change the start date to the next month\n",
    "    start_date += pd.DateOffset(months=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a8bde",
   "metadata": {},
   "source": [
    "### Forecast with decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3cf91e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the trend and seasonal components for a specific series\n",
    "# trend_component = decomp_dict[target_variable]['trend']\n",
    "# seasonal_component = decomp_dict[target_variable]['seasonal']\n",
    "# #residual_component = pd.DataFrame(residual_series[target_variable], index=residual_series.index)\n",
    "# residual_component = residual_series\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d753a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adjust the index to datetime format\n",
    "# df_adjusted.index = pd.to_datetime(df_adjusted.index)\n",
    "\n",
    "# # New list to store the forecasts\n",
    "# monthly_forecasts_v2 = []\n",
    "\n",
    "# # Define a start date for the forecast\n",
    "# start_date = pd.Timestamp(year=2024, month=1, day=1)\n",
    "\n",
    "# for i in range(12):  # For each month in the next year\n",
    "#     # Define a end date for the forecast\n",
    "#     end_date = start_date - pd.DateOffset(months=13)\n",
    "#     # Define a start date 12 months before the end date\n",
    "#     start_date_12_months_before = end_date - pd.DateOffset(months=11)\n",
    "#     # Subset the dataset to get the data for the last 12 months\n",
    "#     data_for_prediction = residual_component.loc[start_date_12_months_before:end_date]\n",
    "#     # Scale the data\n",
    "#     data_for_prediction_scaled = scaler.transform(data_for_prediction)\n",
    "#     data_for_prediction_scaled_df = pd.DataFrame(data_for_prediction_scaled, columns=df_adjusted.columns) # convert to dataframe\n",
    "#     # Include the index in the train and test sets\n",
    "#     data_for_prediction_scaled_df.index = data_for_prediction.index\n",
    "    \n",
    "#     # Prepare the data for the model\n",
    "#     X_predict = np.array([data_for_prediction_scaled_df]) # convert to numpy array\n",
    "#     # Predict the next month\n",
    "#     future_prediction_scaled = model.predict(X_predict)\n",
    "    \n",
    "#     # Reverse the scaling\n",
    "#     temp_array = np.zeros((1, len(df_adjusted.columns))) # Create an array of zeros\n",
    "#     temp_array[0, 0] = future_prediction_scaled[0, 0] # Store the prediction in the first column\n",
    "\n",
    "#     # rescale the prediction\n",
    "#     future_prediction_rescaled = scaler.inverse_transform(temp_array)[0, 0]\n",
    "#     # Append the prediction to the list\n",
    "#     monthly_forecasts_v2.append(future_prediction_rescaled)\n",
    "#     # Change the start date to the next month\n",
    "#     start_date += pd.DateOffset(months=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# # Example: trend_component and seasonal_component are your trend and seasonality series up to Dec/2023\n",
    "\n",
    "# # Extrapolate the Trend\n",
    "# # Convert dates into a numerical format for linear regression\n",
    "# date_nums = np.arange(len(trend_component)).reshape(-1, 1)\n",
    "\n",
    "# # Fit linear regression to the trend component\n",
    "# lin_reg = LinearRegression().fit(date_nums, trend_component.values)\n",
    "\n",
    "# # Predict the trend for the next 12 months\n",
    "# future_dates_num = np.arange(len(trend_component), len(trend_component) + 12).reshape(-1, 1)\n",
    "# future_trend = lin_reg.predict(future_dates_num)\n",
    "\n",
    "# # Project the Seasonality\n",
    "# # Repeat the last known seasonal pattern for 2024\n",
    "# last_year_seasonal = seasonal_component[-12:].values\n",
    "# future_seasonal = np.tile(last_year_seasonal, 1)  # Assuming annual seasonality\n",
    "\n",
    "# # Combine Residuals, Trend, and Seasonality for each month of 2024\n",
    "# future_series = future_trend + future_seasonal + np.array(monthly_forecasts_v2)\n",
    "\n",
    "# # Assuming monthly_forecasts is the list of predicted residuals you've already calculated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "99e6e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the forecasts to a DataFrame\n",
    "# Get the last date of the test set\n",
    "last_test_date = Y_test_rescaled_df.index[-1]\n",
    "future_dates = pd.date_range(start=last_test_date + pd.DateOffset(months=1), periods=12, freq='MS')\n",
    "monthly_forecasts_df = pd.DataFrame(monthly_forecasts, index=future_dates, columns=[target_variable, 'Lower Bound', 'Upper Bound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3d9413d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the monthly forecasts to a DataFrame\n",
    "#monthly_forecasts = pd.DataFrame(monthly_forecasts, index=future_dates, columns=[target_variable])\n",
    "\n",
    "# Convert future_series to a DataFrame\n",
    "#future_series_df = pd.DataFrame(future_series, index=future_dates, columns=[target_variable])\n",
    "\n",
    "# Plot the actuals, test predictions, and future forecasts\n",
    "plt.figure(figsize=(14, 7))\n",
    "# Actuals for the test set\n",
    "#plt.plot(Y_test_rescaled_df.index, Y_test_rescaled_df[target_variable], label='Actuals', marker='o')\n",
    "# Actuals for the test set\n",
    "plt.plot(Y_test_rescaled_df.index, Y_test_rescaled_df[target_variable], label='Actuals', marker='o', color='blue')\n",
    "# Predictions for the test set\n",
    "plt.plot(predictions.index, predictions[target_variable], label='Test Predictions', marker='x', color='orange')\n",
    "\n",
    "# Future forecasts with confidence intervals\n",
    "plt.plot(monthly_forecasts_df.index, monthly_forecasts_df[target_variable], label='Future Forecasts', marker='*', color='red')\n",
    "\n",
    "# Confidence intervals\n",
    "plt.fill_between(monthly_forecasts_df.index, \n",
    "                 monthly_forecasts_df['Lower Bound'], \n",
    "                 monthly_forecasts_df['Upper Bound'], \n",
    "                 color='red', alpha=0.2, label='95% Confidence Interval')\n",
    "\n",
    "# Predictions for the test set\n",
    "#plt.plot(predictions.index, predictions[target_variable], label='Test Predictions', marker='x')\n",
    "# Future forecasts decomposing the original series from the last 12 months and them recomposing after the forecast\n",
    "# plt.plot(future_series_df.index, future_series_df[target_variable], label='Future Forecasts - Decomposing', marker='*')\n",
    "# Future forecasts based on the complete original series from the last 12 months\n",
    "#plt.plot(future_series_df.index, monthly_forecasts, label='Future Forecasts - Complete Series', marker='*')\n",
    "\n",
    "# Annotate the last predicted observation\n",
    "last_predicted_date = predictions.index[-1]\n",
    "last_predicted_value = predictions[target_variable].iloc[-1]\n",
    "plt.scatter(last_predicted_date, last_predicted_value, color='orange')  # Highlight the last point\n",
    "plt.text(last_predicted_date, last_predicted_value, f' R${last_predicted_value:,.2f}', color='orange', va='bottom', ha='right')\n",
    "\n",
    "# # Annotate the last forecast observation - decomposing\n",
    "# last_forecast_date = future_series_df.index[-1]\n",
    "# last_forecast_value = future_series_df[target_variable].iloc[-1]\n",
    "# plt.scatter(last_forecast_date, last_forecast_value, color='green')  # Highlight the last point\n",
    "# plt.text(last_forecast_date, last_forecast_value, f' R${last_forecast_value:,.2f}', color='green', va='bottom', ha='right')\n",
    "\n",
    "# Annotate the last forecast observation - Complete Series\n",
    "last_forecast_date = monthly_forecasts_df.index[-1]\n",
    "last_forecast_value = monthly_forecasts_df[target_variable].iloc[-1]\n",
    "plt.scatter(last_forecast_date, last_forecast_value, color='red')  # Highlight the last point\n",
    "plt.text(last_forecast_date, last_forecast_value, f' R${last_forecast_value:,.2f}', color='red', va='center', ha='left')\n",
    "\n",
    "plt.title('RNN Model Forecast (Static) vs Actuals with Confidence Intervals')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel(target_variable)\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
