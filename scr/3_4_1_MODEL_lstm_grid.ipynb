{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "27e86681",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27e86681",
    "outputId": "1af8a1db-879d-44d0-b1df-07a82eaa1fb8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# #Let's Examine Correlation between the series\n",
    "# ###https://github.com/mikekeith52/scalecast-examples/blob/main/multivariate/multivariate.ipynb\n",
    "\n",
    "import useful_functions as uf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.regularizers import l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cs469n2XOr_-",
   "metadata": {
    "id": "cs469n2XOr_-"
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "file_path = '../data/data_orig_parameters.csv'\n",
    "#file_path = '../data/BR_param_EDA.csv'\n",
    "#file_path = '../data/data_cleaned_RF.csv'\n",
    "#file_path = '../data/data_cleaned_LASSO.csv'\n",
    "#file_path = '../data/data_cleaned_RFE.csv'\n",
    "# parse the date column and set it as the index of the dataframe\n",
    "df_raw = pd.read_csv(file_path, parse_dates=['Date'], index_col='Date')\n",
    "# define the target variable as the first column\n",
    "target_variable = df_raw.columns[0]\n",
    "# Convert all columns to float\n",
    "df_raw = df_raw.astype('float64')\n",
    "\n",
    "# Remove outliers\n",
    "remove_outliers_threshold = np.nan\n",
    "#remove_outliers_threshold = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e06252c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any NA value\n",
    "#print(df_raw.isna().sum())\n",
    "\n",
    "# If we want to remove outliers\n",
    "if not pd.isna(remove_outliers_threshold):\n",
    "    df_cleaned = uf.remove_outliers(df_raw.copy(), threshold=remove_outliers_threshold)\n",
    "else:\n",
    "    df_cleaned = df_raw.copy()\n",
    "\n",
    "# Fill missing values\n",
    "df_adjusted = uf.fill_missing_values(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c4bc94dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is any collum with unique values and drop it\n",
    "for column in df_adjusted.columns:\n",
    "    if len(df_adjusted[column].unique()) == 1:\n",
    "        df_adjusted.drop(column, axis=1, inplace=True)\n",
    "\n",
    "#print(df_adjusted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b5ef4449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test and validation set sizes\n",
    "val_size = 48 # 48 months or 4 years\n",
    "test_size = 48 # 48 months or 4 years\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_raw_total = df_adjusted.copy()[:-test_size] # This total trainning set will be used to train the final model\n",
    "train_raw = train_raw_total[:-val_size]\n",
    "val_raw = train_raw_total[-val_size:]\n",
    "test_raw = df_adjusted.copy()[-test_size:]\n",
    "\n",
    "# # FIll missing values\n",
    "\n",
    "df_train = uf.fill_missing_values(train_raw)\n",
    "df_val = uf.fill_missing_values(val_raw)\n",
    "df_test = uf.fill_missing_values(test_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "db93b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LetÂ´s scale the dfs\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_train = scaler.fit_transform(df_train)\n",
    "scaled_val = scaler.transform(df_val)\n",
    "scaled_test = scaler.transform(df_test)\n",
    "# include df columns names in the train and test sets\n",
    "train = pd.DataFrame(scaled_train, columns=df_train.columns)\n",
    "val = pd.DataFrame(scaled_val, columns=df_val.columns)\n",
    "test = pd.DataFrame(scaled_test, columns=df_test.columns)\n",
    "# Include the index in the train and test sets\n",
    "train.index = df_train.index\n",
    "val.index = df_val.index\n",
    "test.index = df_test.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f39293",
   "metadata": {},
   "source": [
    "Reshape your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e25733e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the series to samples\n",
    "# We will use the past 12 months to predict the next 12 months\n",
    "def createXY(dataset, n_past, n_future):\n",
    "    dataX, dataY = [], []\n",
    "    # Loop for the entire dataset\n",
    "    for i in range(n_past, len(dataset) - n_future + 1):\n",
    "        dataX.append(dataset.iloc[i - n_past:i].values)  # Past n months\n",
    "        dataY.append(dataset.iloc[i + n_future - 1, 0])  #\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "n_past = 12  # Number of past months to use\n",
    "n_future = 12  # Number of future months to predict\n",
    "\n",
    "# Create the samples\n",
    "X_train, Y_train = createXY(train, n_past, n_future)\n",
    "X_val, Y_val = createXY(val, n_past, n_future)\n",
    "X_test, Y_test = createXY(test, n_past, n_future)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3f8c6",
   "metadata": {},
   "source": [
    "### Let's grid to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "15a5db0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  2 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  3 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  4 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  5 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  6 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  7 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  8 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  9 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  10 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  11 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  12 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  13 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  14 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  15 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  16 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  17 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  18 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  19 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  20 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  21 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  22 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  23 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  24 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  25 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  26 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  27 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  28 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  29 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  30 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  31 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  32 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  33 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  34 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  35 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  36 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  37 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  38 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  39 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  40 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  41 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  42 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  43 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  44 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  45 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  46 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  47 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  48 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  49 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  50 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  51 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  52 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  53 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  54 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  55 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  56 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  57 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  58 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  59 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  60 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  61 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  62 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  63 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  64 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  65 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  66 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  67 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  68 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  69 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  70 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  71 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  72 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  73 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  74 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  75 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  76 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  77 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  78 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  79 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  80 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  81 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  82 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  83 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  84 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  85 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  86 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  87 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  88 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  89 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  90 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  91 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  92 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  93 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  94 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  95 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  96 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  97 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  98 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  99 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  100 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  101 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  102 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  103 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  104 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  105 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  106 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  107 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  108 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  109 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  110 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  111 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  112 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  113 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  114 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  115 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  116 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  117 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  118 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  119 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  120 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  121 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  122 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  123 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  124 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  125 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  126 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  127 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  128 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  129 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  130 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  131 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  132 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  133 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  134 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  135 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  136 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  137 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  138 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  139 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  140 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  141 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  142 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  143 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  144 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  145 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  146 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  147 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  148 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  149 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  150 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  151 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  152 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  153 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  154 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  155 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  156 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  157 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  158 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  159 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  160 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  161 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  162 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  163 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  164 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  165 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  166 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  167 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  168 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  169 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  170 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  171 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  172 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  173 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  174 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  175 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  176 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  177 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  178 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  179 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  180 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  181 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  182 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  183 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  184 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  185 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  186 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  187 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  188 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  189 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  190 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  191 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  192 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  193 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  194 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  195 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  196 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  197 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  198 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  199 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  200 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  201 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  202 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  203 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  204 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  205 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  206 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  207 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  208 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  209 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  210 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  211 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  212 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  213 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  214 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  215 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  216 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  217 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  218 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  219 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  220 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  221 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  222 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  223 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  224 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  225 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  226 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  227 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  228 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  229 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  230 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  231 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  232 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  233 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  234 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  235 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  236 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  237 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  238 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  239 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  240 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  241 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  242 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  243 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  244 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  245 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  246 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  247 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  248 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  249 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  250 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  251 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  252 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  253 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  254 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  255 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  256 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  257 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  258 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  259 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  260 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  261 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  262 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  263 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  264 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  265 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  266 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  267 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  268 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  269 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  270 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  271 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  272 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  273 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  274 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  275 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  276 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  277 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  278 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  279 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  280 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  281 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  282 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  283 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  284 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  285 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  286 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  287 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  288 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  289 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  290 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  291 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  292 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  293 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  294 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  295 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  296 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  297 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  298 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  299 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  300 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  301 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  302 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  303 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  304 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  305 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  306 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  307 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  308 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  309 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  310 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  311 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  312 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  313 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  314 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  315 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  316 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  317 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  318 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  319 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  320 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  321 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  322 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  323 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  324 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  325 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  326 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  327 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  328 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  329 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  330 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  331 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  332 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  333 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  334 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  335 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  336 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  337 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  338 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  339 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  340 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  341 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  342 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  343 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  344 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  345 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  346 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  347 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  348 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  349 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  350 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  351 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  352 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  353 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  354 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  355 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  356 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  357 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  358 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  359 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  360 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  361 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  362 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  363 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  364 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  365 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  366 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  367 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  368 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  369 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  370 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  371 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  372 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  373 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  374 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  375 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  376 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  377 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  378 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  379 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  380 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  381 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  382 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  383 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  384 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  385 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  386 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  387 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  388 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  389 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  390 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  391 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  392 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  393 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  394 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  395 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  396 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  397 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  398 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  399 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  400 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  401 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  402 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  403 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  404 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  405 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  406 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  407 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  408 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  409 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  410 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  411 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  412 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  413 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  414 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  415 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  416 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  417 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  418 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  419 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  420 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  421 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  422 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  423 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  424 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  425 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  426 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  427 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  428 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  429 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  430 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  431 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  432 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  433 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  434 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  435 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  436 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  437 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  438 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  439 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  440 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  441 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  442 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  443 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  444 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  445 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  446 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  447 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  448 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  449 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  450 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  451 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  452 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  453 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  454 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  455 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  456 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  457 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  458 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  459 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  460 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  461 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  462 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  463 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  464 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  465 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  466 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  467 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  468 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  469 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  470 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  471 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  472 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  473 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  474 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  475 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  476 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  477 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  478 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  479 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  480 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  481 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  482 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  483 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  484 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  485 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  486 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  487 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  488 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  489 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  490 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  491 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  492 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  493 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  494 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  495 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  496 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  497 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  498 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  499 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  500 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  501 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  502 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  503 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  504 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  505 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  506 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  507 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  508 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  509 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  510 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  511 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  512 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  513 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  514 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  515 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  516 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  517 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  518 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  519 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  520 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  521 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  522 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  523 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  524 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  525 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  526 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  527 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  528 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  529 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  530 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  531 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  532 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  533 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  534 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  535 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  536 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  537 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  538 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  539 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  540 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  541 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  542 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  543 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  544 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  545 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  546 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  547 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  548 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  549 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  550 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  551 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  552 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  553 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  554 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  555 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  556 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  557 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  558 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  559 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  560 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  561 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  562 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  563 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  564 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  565 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  566 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  567 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  568 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  569 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  570 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  571 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  572 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  573 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  574 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  575 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  576 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  577 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  578 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  579 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  580 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  581 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  582 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  583 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  584 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  585 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  586 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  587 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  588 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  589 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  590 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  591 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  592 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  593 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  594 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  595 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  596 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  597 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  598 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  599 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  600 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  601 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  602 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  603 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  604 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  605 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  606 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  607 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  608 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  609 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  610 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  611 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  612 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  613 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  614 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  615 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  616 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  617 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  618 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  619 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  620 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  621 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  622 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  623 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  624 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  625 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  626 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  627 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  628 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  629 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  630 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  631 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  632 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  633 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  634 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  635 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  636 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  637 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  638 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  639 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  640 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  641 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  642 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  643 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  644 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  645 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  646 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  647 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  648 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  649 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  650 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  651 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  652 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  653 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  654 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  655 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  656 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  657 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  658 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  659 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  660 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  661 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  662 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  663 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  664 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  665 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  666 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  667 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  668 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  669 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  670 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  671 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  672 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  673 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  674 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  675 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  676 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  677 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  678 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  679 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  680 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  681 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  682 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  683 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  684 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  685 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  686 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  687 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  688 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  689 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  690 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  691 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  692 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  693 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  694 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  695 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  696 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  697 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  698 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  699 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  700 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  701 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  702 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  703 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  704 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  705 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  706 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  707 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  708 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  709 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  710 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  711 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  712 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  713 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  714 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  715 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  716 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  717 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  718 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  719 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  720 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  721 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  722 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  723 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  724 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  725 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  726 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  727 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  728 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  729 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  730 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  731 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  732 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  733 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  734 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  735 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  736 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  737 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  738 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  739 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  740 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  741 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  742 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  743 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  744 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  745 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  746 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  747 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  748 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  749 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  750 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  751 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  752 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  753 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  754 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  755 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  756 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  757 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  758 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  759 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  760 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  761 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  762 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  763 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  764 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  765 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  766 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  767 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  768 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  769 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  770 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  771 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  772 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  773 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  774 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  775 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  776 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  777 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  778 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  779 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  780 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  781 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  782 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  783 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  784 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  785 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  786 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  787 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  788 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  789 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  790 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  791 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  792 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  793 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  794 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  795 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  796 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  797 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  798 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  799 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  800 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  801 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  802 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  803 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  804 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  805 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  806 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  807 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  808 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  809 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  810 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  811 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  812 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  813 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  814 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  815 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  816 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  817 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  818 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  819 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  820 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  821 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  822 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  823 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  824 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  825 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  826 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  827 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  828 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  829 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  830 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  831 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  832 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  833 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  834 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  835 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  836 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  837 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  838 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  839 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  840 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  841 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  842 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  843 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  844 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  845 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  846 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  847 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  848 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  849 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  850 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  851 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  852 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  853 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  854 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  855 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  856 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  857 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  858 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  859 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  860 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  861 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  862 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  863 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  864 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  865 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  866 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  867 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  868 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  869 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  870 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  871 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  872 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  873 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  874 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  875 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  876 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  877 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  878 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  879 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  880 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  881 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  882 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  883 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  884 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  885 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  886 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  887 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  888 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  889 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  890 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  891 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  892 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  893 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  894 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  895 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  896 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  897 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  898 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  899 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  900 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  901 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  902 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  903 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  904 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  905 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  906 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  907 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  908 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  909 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  910 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  911 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  912 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  913 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  914 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  915 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  916 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  917 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  918 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  919 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  920 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  921 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  922 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  923 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  924 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  925 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  926 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  927 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  928 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  929 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  930 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  931 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  932 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  933 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  934 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  935 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  936 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  937 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  938 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  939 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  940 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  941 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  942 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  943 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  944 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  945 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  946 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  947 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  948 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  949 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  950 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  951 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  952 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  953 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  954 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  955 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  956 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  957 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  958 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  959 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  960 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  961 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  962 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  963 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  964 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  965 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  966 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  967 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  968 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  969 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  970 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  971 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  972 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  973 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  974 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  975 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  976 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  977 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  978 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  979 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  980 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  981 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  982 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  983 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  984 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  985 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  986 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  987 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  988 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  989 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  990 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  991 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  992 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  993 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  994 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  995 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  996 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  997 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  998 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  999 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1000 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1001 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1002 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1003 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1004 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1005 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1006 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1007 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1008 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1009 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1010 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1011 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1012 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1013 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1014 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1015 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1016 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1017 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1018 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1019 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1020 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1021 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1022 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1023 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1024 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1025 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1026 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1027 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1028 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1029 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1030 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1031 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1032 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1033 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1034 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1035 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1036 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1037 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1038 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1039 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1040 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1041 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1042 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1043 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1044 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1045 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1046 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1047 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1048 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1049 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1050 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1051 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1052 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1053 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1054 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1055 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1056 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1057 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1058 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1059 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1060 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1061 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1062 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1063 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1064 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1065 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1066 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1067 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1068 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1069 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1070 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1071 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1072 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1073 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1074 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1075 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1076 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1077 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1078 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1079 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1080 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1081 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1082 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1083 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1084 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1085 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1086 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1087 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1088 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1089 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1090 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1091 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1092 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1093 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1094 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1095 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1096 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1097 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1098 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1099 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1100 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1101 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1102 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1103 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1104 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1105 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1106 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1107 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1108 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1109 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1110 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1111 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1112 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1113 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1114 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1115 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1116 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1117 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1118 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1119 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1120 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1121 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1122 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1123 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1124 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1125 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1126 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1127 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1128 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1129 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1130 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1131 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1132 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1133 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1134 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1135 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1136 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1137 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1138 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1139 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1140 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1141 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1142 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1143 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1144 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1145 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1146 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1147 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1148 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1149 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1150 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1151 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1152 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1153 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1154 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1155 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1156 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1157 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1158 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1159 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1160 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1161 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1162 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1163 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1164 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1165 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1166 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1167 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1168 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1169 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1170 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1171 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1172 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1173 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1174 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1175 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1176 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1177 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1178 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1179 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1180 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1181 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1182 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1183 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1184 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1185 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1186 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1187 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1188 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1189 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1190 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1191 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1192 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1193 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1194 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1195 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1196 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1197 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1198 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1199 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1200 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1201 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1202 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1203 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1204 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1205 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1206 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1207 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1208 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1209 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1210 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1211 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1212 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1213 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1214 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1215 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1216 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1217 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1218 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1219 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1220 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1221 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1222 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1223 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1224 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1225 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1226 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1227 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1228 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1229 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1230 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1231 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1232 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1233 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1234 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1235 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1236 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1237 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1238 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1239 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1240 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1241 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1242 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1243 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1244 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1245 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1246 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1247 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1248 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1249 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1250 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1251 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1252 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1253 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1254 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1255 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1256 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1257 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1258 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1259 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1260 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1261 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1262 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1263 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1264 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1265 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1266 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1267 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1268 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1269 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1270 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1271 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1272 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1273 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1274 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1275 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1276 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1277 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1278 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1279 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1280 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1281 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1282 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1283 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1284 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1285 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1286 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1287 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1288 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1289 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1290 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1291 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1292 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1293 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1294 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1295 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1296 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1297 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1298 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1299 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1300 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1301 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1302 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1303 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1304 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1305 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1306 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1307 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1308 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1309 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1310 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1311 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1312 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1313 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1314 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1315 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1316 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1317 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1318 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1319 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1320 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1321 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1322 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1323 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1324 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1325 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1326 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1327 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1328 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1329 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1330 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1331 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1332 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1333 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1334 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1335 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1336 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1337 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1338 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1339 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1340 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1341 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1342 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1343 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1344 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1345 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1346 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1347 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1348 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1349 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1350 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1351 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1352 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1353 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1354 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1355 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1356 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1357 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1358 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1359 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1360 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1361 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1362 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1363 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1364 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1365 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1366 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1367 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1368 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1369 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1370 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1371 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1372 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1373 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1374 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1375 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1376 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1377 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1378 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1379 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1380 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1381 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1382 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1383 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1384 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1385 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1386 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1387 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1388 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1389 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1390 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1391 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1392 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1393 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1394 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1395 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1396 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1397 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1398 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1399 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1400 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1401 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1402 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1403 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1404 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1405 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1406 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1407 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1408 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1409 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1410 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1411 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1412 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1413 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1414 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1415 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1416 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1417 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1418 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1419 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1420 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1421 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1422 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1423 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1424 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1425 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1426 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1427 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1428 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1429 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1430 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1431 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1432 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1433 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1434 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1435 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1436 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1437 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1438 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1439 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1440 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1441 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1442 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1443 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1444 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1445 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1446 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1447 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1448 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1449 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1450 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1451 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1452 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1453 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1454 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1455 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1456 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1457 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1458 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1459 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1460 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1461 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1462 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1463 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1464 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1465 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1466 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1467 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1468 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1469 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1470 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1471 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1472 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1473 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1474 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1475 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1476 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1477 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1478 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1479 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1480 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1481 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1482 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1483 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1484 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1485 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1486 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1487 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1488 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1489 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1490 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1491 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1492 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1493 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1494 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1495 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1496 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1497 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1498 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1499 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1500 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1501 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1502 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1503 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1504 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1505 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1506 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1507 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1508 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1509 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1510 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1511 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1512 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1513 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1514 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1515 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1516 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1517 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1518 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1519 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1520 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1521 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1522 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1523 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1524 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1525 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1526 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1527 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1528 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1529 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1530 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1531 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1532 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1533 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1534 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1535 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1536 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1537 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1538 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1539 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1540 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1541 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1542 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1543 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1544 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1545 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1546 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1547 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1548 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1549 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1550 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1551 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1552 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1553 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1554 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1555 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1556 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1557 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1558 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1559 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1560 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1561 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1562 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1563 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1564 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1565 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1566 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1567 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1568 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1569 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1570 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1571 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1572 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1573 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1574 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1575 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1576 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1577 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1578 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1579 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1580 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1581 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1582 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1583 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1584 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1585 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1586 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1587 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1588 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1589 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1590 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1591 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1592 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1593 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1594 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1595 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1596 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1597 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1598 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1599 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1600 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1601 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1602 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1603 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1604 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1605 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1606 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1607 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1608 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1609 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1610 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1611 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1612 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1613 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1614 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1615 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1616 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1617 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1618 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1619 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1620 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1621 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1622 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1623 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1624 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1625 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1626 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1627 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1628 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1629 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1630 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1631 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1632 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1633 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1634 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1635 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1636 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1637 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1638 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1639 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1640 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1641 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1642 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1643 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1644 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1645 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1646 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1647 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1648 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1649 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1650 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1651 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1652 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1653 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1654 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1655 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1656 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1657 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1658 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1659 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1660 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1661 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1662 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1663 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1664 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1665 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1666 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1667 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1668 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1669 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1670 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1671 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1672 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1673 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1674 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1675 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1676 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1677 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1678 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1679 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1680 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1681 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1682 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1683 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1684 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1685 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1686 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1687 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1688 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1689 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1690 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1691 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1692 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1693 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1694 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1695 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1696 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1697 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1698 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1699 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1700 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1701 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1702 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1703 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1704 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1705 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1706 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1707 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1708 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1709 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1710 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1711 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1712 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1713 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1714 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1715 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1716 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1717 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1718 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1719 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1720 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1721 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1722 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1723 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1724 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1725 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1726 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1727 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1728 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1729 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1730 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1731 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1732 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1733 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1734 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1735 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1736 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1737 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1738 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1739 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1740 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1741 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1742 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1743 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1744 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1745 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1746 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1747 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1748 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1749 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1750 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1751 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1752 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1753 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1754 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1755 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1756 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1757 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1758 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1759 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1760 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1761 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1762 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1763 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1764 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1765 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1766 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1767 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1768 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1769 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1770 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1771 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1772 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1773 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1774 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1775 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1776 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1777 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1778 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1779 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1780 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1781 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1782 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1783 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1784 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1785 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1786 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1787 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1788 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1789 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1790 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1791 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1792 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1793 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1794 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1795 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1796 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1797 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1798 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1799 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1800 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1801 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1802 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1803 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1804 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1805 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1806 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1807 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1808 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1809 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1810 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1811 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1812 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1813 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1814 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1815 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1816 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1817 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1818 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1819 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1820 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1821 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1822 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1823 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1824 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1825 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1826 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1827 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1828 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1829 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1830 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1831 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1832 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1833 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1834 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1835 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1836 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1837 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1838 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1839 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1840 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1841 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1842 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1843 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1844 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1845 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1846 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1847 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1848 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1849 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1850 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1851 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1852 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1853 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1854 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1855 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1856 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1857 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1858 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1859 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1860 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1861 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1862 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1863 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1864 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1865 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1866 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1867 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1868 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1869 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1870 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1871 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.1, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1872 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1873 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1874 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1875 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1876 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1877 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1878 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1879 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1880 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1881 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1882 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1883 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1884 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1885 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1886 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1887 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1888 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1889 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1890 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1891 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1892 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1893 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1894 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1895 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1896 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1897 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1898 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1899 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1900 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1901 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1902 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1903 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1904 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1905 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1906 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1907 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1908 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1909 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1910 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1911 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1912 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1913 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1914 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1915 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1916 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1917 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1918 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1919 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1920 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1921 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1922 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1923 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1924 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1925 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1926 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1927 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1928 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1929 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1930 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1931 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1932 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1933 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1934 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1935 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1936 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1937 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1938 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1939 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1940 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1941 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1942 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1943 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1944 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1945 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1946 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1947 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1948 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1949 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1950 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1951 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1952 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1953 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1954 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1955 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1956 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1957 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1958 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1959 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1960 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1961 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1962 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1963 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1964 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1965 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1966 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1967 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1968 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1969 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1970 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1971 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1972 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1973 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1974 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1975 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1976 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1977 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1978 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1979 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1980 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1981 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1982 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1983 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1984 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1985 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1986 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1987 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1988 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1989 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1990 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1991 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1992 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1993 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  1994 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  1995 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  1996 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  1997 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  1998 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  1999 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  2000 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  2001 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  2002 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  2003 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  2004 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  2005 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  2006 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  2007 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  2008 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  2009 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  2010 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  2011 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  2012 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  2013 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  2014 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  2015 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 75, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  2016 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  2017 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  2018 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  2019 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  2020 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  2021 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  2022 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  2023 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  2024 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  2025 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  2026 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  2027 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  2028 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  2029 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  2030 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  2031 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  2032 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  2033 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  2034 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  2035 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  2036 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  2037 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  2038 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  2039 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  2040 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  2041 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  2042 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  2043 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  2044 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  2045 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  2046 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  2047 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  2048 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  2049 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  2050 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  2051 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  2052 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  2053 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  2054 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  2055 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  2056 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  2057 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 100, 2: 100, 3: 50}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  2058 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  2059 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  2060 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  2061 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  2062 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  2063 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 20, 'learning_rate': 0.0001, 'lstm_units': {1: 200, 2: 200, 3: 100}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  2064 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  2065 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  2066 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  2067 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  2068 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'adam'} . This is the  2069 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 10, 2: 10, 3: 5}, 'n_layers': 3, 'optimizer': 'rmsprop'} . This is the  2070 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'adam'} . This is the  2071 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 1, 'optimizer': 'rmsprop'} . This is the  2072 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'adam'} . This is the  2073 th iteration\n",
      "Applying parameters:  {'alphas_l1_l2': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'learning_rate': 0.001, 'lstm_units': {1: 50, 2: 30, 3: 15}, 'n_layers': 2, 'optimizer': 'rmsprop'} . This is the  2074 th iteration\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplying parameters: \u001b[39m\u001b[38;5;124m\"\u001b[39m, params,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. This is the \u001b[39m\u001b[38;5;124m\"\u001b[39m, interactions, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mth iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     73\u001b[0m interactions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 74\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m<\u001b[39m best_score:\n\u001b[0;32m     76\u001b[0m     best_score \u001b[38;5;241m=\u001b[39m score\n",
      "Cell \u001b[1;32mIn[70], line 44\u001b[0m, in \u001b[0;36mcustom_fit\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     41\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# Stop training when the validation loss is no longer decreasing after X epochs\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Fitting the model with early stopping\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Compute the loss on the validation set\u001b[39;00m\n\u001b[0;32m     51\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_val, Y_val, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mg:\\My Drive\\Data_Projects\\MDS\\master_thesis\\fiscal-balance-forecast\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mg:\\My Drive\\Data_Projects\\MDS\\master_thesis\\fiscal-balance-forecast\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:325\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    324\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 325\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[0;32m    327\u001b[0m         step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    328\u001b[0m     )\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mg:\\My Drive\\Data_Projects\\MDS\\master_thesis\\fiscal-balance-forecast\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mg:\\My Drive\\Data_Projects\\MDS\\master_thesis\\fiscal-balance-forecast\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mg:\\My Drive\\Data_Projects\\MDS\\master_thesis\\fiscal-balance-forecast\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mg:\\My Drive\\Data_Projects\\MDS\\master_thesis\\fiscal-balance-forecast\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\My Drive\\Data_Projects\\MDS\\master_thesis\\fiscal-balance-forecast\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mg:\\My Drive\\Data_Projects\\MDS\\master_thesis\\fiscal-balance-forecast\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mg:\\My Drive\\Data_Projects\\MDS\\master_thesis\\fiscal-balance-forecast\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mg:\\My Drive\\Data_Projects\\MDS\\master_thesis\\fiscal-balance-forecast\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mg:\\My Drive\\Data_Projects\\MDS\\master_thesis\\fiscal-balance-forecast\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Build a LSTM model testing different parameters\n",
    "\n",
    "\n",
    "# Function to build the model\n",
    "def build_model(n_layers = 2, optimizer='adam', learning_rate=0.001, \n",
    "                lstm_units={0: 50,  1: 20, 2: 10},\n",
    "                dropout_rate=0.2, alphas_l1_l2=0.01):\n",
    "    # Check the optimizer\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model = Sequential()\n",
    "    # Define the input layer shape\n",
    "    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    # Add the LSTM layers\n",
    "    for i in range(n_layers):\n",
    "        return_sequences = i < n_layers - 1  # Only the last layer returns a single vector\n",
    "        model.add(LSTM(units=lstm_units[i+1], \n",
    "                       return_sequences=return_sequences,\n",
    "                       kernel_regularizer=l1_l2(l1=alphas_l1_l2, l2=alphas_l1_l2),\n",
    "                       recurrent_regularizer=l1_l2(l1=alphas_l1_l2, l2=alphas_l1_l2),\n",
    "                       bias_regularizer=l1_l2(l1=alphas_l1_l2, l2=alphas_l1_l2)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(units=1))  # Output layer\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Function to test each set of parameters\n",
    "def custom_fit(params):\n",
    "    training_params = {key: params[key] for key in params if key in ['n_layers', 'alphas_l1_l2',\n",
    "                                                                     'dropout_rate',\n",
    "                                                                     'lstm_units', \n",
    "                                                                     'optimizer', \n",
    "                                                                     'learning_rate']} # Use this to pass the parameters to the model\n",
    "    model = build_model(**training_params) # Create the model\n",
    "    \n",
    "    # Define EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True) # Stop training when the validation loss is no longer decreasing after X epochs\n",
    "\n",
    "    # Fitting the model with early stopping\n",
    "    model.fit(X_train, Y_train, \n",
    "              epochs=params['epochs'], \n",
    "              batch_size=params['batch_size'], \n",
    "              verbose=0,\n",
    "              validation_data=(X_val, Y_val), \n",
    "              callbacks=[early_stopping])\n",
    "    # Compute the loss on the validation set\n",
    "    loss = model.evaluate(X_val, Y_val, verbose=0)\n",
    "    return loss\n",
    "\n",
    "# EspaÃ§o de parÃ¢metros\n",
    "param_grid = {\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'alphas_l1_l2' : [0.001, 0.01, 0.1, 1],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'epochs': [20, 50, 75],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'learning_rate': [0.001, 0.0001], # Learning rate\n",
    "    'n_layers': [1,2,3], # Number of hidden layers\n",
    "    'lstm_units' : [{1: 10,  2: 10, 3: 5}, {1: 50,  2: 30, 3: 15}, \n",
    "                   {1: 100,  2: 100, 3: 50}, {1: 200,  2: 200, 3: 100}]  \n",
    "}\n",
    "\n",
    "# Compare the scores for each set of parameters\n",
    "best_score = np.inf\n",
    "best_params = None\n",
    "interactions = 1\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(\"Applying parameters: \", params,\". This is the \", interactions, \"th iteration\")\n",
    "    interactions += 1\n",
    "    score = custom_fit(params)\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "\n",
    "print(f'Best Score: {best_score}')\n",
    "print(f'Best Parameters: {best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b6ee7616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alphas_l1_l2': 0.001,\n",
       " 'batch_size': 32,\n",
       " 'dropout_rate': 0.3,\n",
       " 'epochs': 75,\n",
       " 'learning_rate': 0.001,\n",
       " 'lstm_units': {1: 10, 2: 10, 3: 5},\n",
       " 'n_layers': 2,\n",
       " 'optimizer': 'rmsprop'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_bkp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d59d3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's build the model with the best parameters and train it\n",
    "# Get epochs and batch_size from best_params\n",
    "best_params_bkp = best_params.copy()\n",
    "epochs = best_params.pop('epochs')\n",
    "batch_size = best_params.pop('batch_size')\n",
    "\n",
    "# build the model\n",
    "best_model = build_model(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5a90bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "best_model.save('best_lstm_model_grid_ori_6.keras')  # Saves the model to a HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c1b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we need to restore the best model\n",
    "\n",
    "# # Load the model from the file\n",
    "# best_model = load_model('best_lstm_model_grid.keras')\n",
    "# # from manual hyperparameter tuning\n",
    "# epochs = 20\n",
    "# batch_size = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "544e7993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 - 5s - 918ms/step - loss: 0.7346 - val_loss: 0.6650\n",
      "Epoch 2/75\n",
      "5/5 - 1s - 160ms/step - loss: 0.6727 - val_loss: 0.6354\n",
      "Epoch 3/75\n",
      "5/5 - 1s - 163ms/step - loss: 0.6424 - val_loss: 0.6120\n",
      "Epoch 4/75\n",
      "5/5 - 1s - 161ms/step - loss: 0.6138 - val_loss: 0.6197\n",
      "Epoch 5/75\n",
      "5/5 - 1s - 156ms/step - loss: 0.5948 - val_loss: 0.5815\n",
      "Epoch 6/75\n",
      "5/5 - 1s - 154ms/step - loss: 0.5844 - val_loss: 0.5678\n",
      "Epoch 7/75\n",
      "5/5 - 1s - 156ms/step - loss: 0.5617 - val_loss: 0.5597\n",
      "Epoch 8/75\n",
      "5/5 - 1s - 159ms/step - loss: 0.5477 - val_loss: 0.5346\n",
      "Epoch 9/75\n",
      "5/5 - 1s - 154ms/step - loss: 0.5266 - val_loss: 0.5328\n",
      "Epoch 10/75\n",
      "5/5 - 1s - 156ms/step - loss: 0.5040 - val_loss: 0.5121\n",
      "Epoch 11/75\n",
      "5/5 - 1s - 154ms/step - loss: 0.4912 - val_loss: 0.4932\n",
      "Epoch 12/75\n",
      "5/5 - 1s - 164ms/step - loss: 0.4847 - val_loss: 0.4820\n",
      "Epoch 13/75\n",
      "5/5 - 1s - 156ms/step - loss: 0.4689 - val_loss: 0.4623\n",
      "Epoch 14/75\n",
      "5/5 - 1s - 151ms/step - loss: 0.4495 - val_loss: 0.4557\n",
      "Epoch 15/75\n",
      "5/5 - 1s - 160ms/step - loss: 0.4384 - val_loss: 0.4425\n",
      "Epoch 16/75\n",
      "5/5 - 1s - 156ms/step - loss: 0.4214 - val_loss: 0.4298\n",
      "Epoch 17/75\n",
      "5/5 - 1s - 158ms/step - loss: 0.4109 - val_loss: 0.4188\n",
      "Epoch 18/75\n",
      "5/5 - 1s - 166ms/step - loss: 0.3989 - val_loss: 0.4192\n",
      "Epoch 19/75\n",
      "5/5 - 1s - 153ms/step - loss: 0.3794 - val_loss: 0.3946\n",
      "Epoch 20/75\n",
      "5/5 - 1s - 153ms/step - loss: 0.3781 - val_loss: 0.3971\n",
      "Epoch 21/75\n",
      "5/5 - 1s - 155ms/step - loss: 0.3541 - val_loss: 0.3729\n",
      "Epoch 22/75\n",
      "5/5 - 1s - 151ms/step - loss: 0.3472 - val_loss: 0.3733\n",
      "Epoch 23/75\n",
      "5/5 - 1s - 157ms/step - loss: 0.3375 - val_loss: 0.3506\n",
      "Epoch 24/75\n",
      "5/5 - 1s - 159ms/step - loss: 0.3334 - val_loss: 0.3441\n",
      "Epoch 25/75\n",
      "5/5 - 1s - 155ms/step - loss: 0.3142 - val_loss: 0.3231\n",
      "Epoch 26/75\n",
      "5/5 - 1s - 156ms/step - loss: 0.3032 - val_loss: 0.3229\n",
      "Epoch 27/75\n",
      "5/5 - 1s - 154ms/step - loss: 0.3029 - val_loss: 0.3109\n",
      "Epoch 28/75\n",
      "5/5 - 1s - 179ms/step - loss: 0.2843 - val_loss: 0.2971\n",
      "Epoch 29/75\n",
      "5/5 - 1s - 194ms/step - loss: 0.2777 - val_loss: 0.2874\n",
      "Epoch 30/75\n",
      "5/5 - 1s - 186ms/step - loss: 0.2656 - val_loss: 0.2857\n",
      "Epoch 31/75\n",
      "5/5 - 1s - 163ms/step - loss: 0.2582 - val_loss: 0.2848\n",
      "Epoch 32/75\n",
      "5/5 - 1s - 184ms/step - loss: 0.2553 - val_loss: 0.2743\n",
      "Epoch 33/75\n",
      "5/5 - 1s - 198ms/step - loss: 0.2464 - val_loss: 0.2511\n",
      "Epoch 34/75\n",
      "5/5 - 1s - 180ms/step - loss: 0.2367 - val_loss: 0.2471\n",
      "Epoch 35/75\n",
      "5/5 - 1s - 179ms/step - loss: 0.2253 - val_loss: 0.2412\n",
      "Epoch 36/75\n",
      "5/5 - 1s - 163ms/step - loss: 0.2216 - val_loss: 0.2507\n",
      "Epoch 37/75\n",
      "5/5 - 1s - 174ms/step - loss: 0.2110 - val_loss: 0.2329\n",
      "Epoch 38/75\n",
      "5/5 - 1s - 195ms/step - loss: 0.2070 - val_loss: 0.2314\n",
      "Epoch 39/75\n",
      "5/5 - 1s - 184ms/step - loss: 0.1941 - val_loss: 0.2270\n",
      "Epoch 40/75\n",
      "5/5 - 1s - 157ms/step - loss: 0.1914 - val_loss: 0.2362\n",
      "Epoch 41/75\n",
      "5/5 - 1s - 154ms/step - loss: 0.1906 - val_loss: 0.2050\n",
      "Epoch 42/75\n",
      "5/5 - 1s - 156ms/step - loss: 0.1784 - val_loss: 0.2016\n",
      "Epoch 43/75\n",
      "5/5 - 1s - 153ms/step - loss: 0.1770 - val_loss: 0.2050\n",
      "Epoch 44/75\n",
      "5/5 - 1s - 156ms/step - loss: 0.1663 - val_loss: 0.2042\n",
      "Epoch 45/75\n",
      "5/5 - 1s - 152ms/step - loss: 0.1633 - val_loss: 0.1976\n",
      "Epoch 46/75\n",
      "5/5 - 1s - 153ms/step - loss: 0.1589 - val_loss: 0.1748\n",
      "Epoch 47/75\n",
      "5/5 - 1s - 159ms/step - loss: 0.1533 - val_loss: 0.1928\n",
      "Epoch 48/75\n",
      "5/5 - 1s - 156ms/step - loss: 0.1505 - val_loss: 0.1759\n",
      "Epoch 49/75\n",
      "5/5 - 1s - 151ms/step - loss: 0.1476 - val_loss: 0.1816\n",
      "Epoch 50/75\n",
      "5/5 - 1s - 153ms/step - loss: 0.1438 - val_loss: 0.1671\n",
      "Epoch 51/75\n",
      "5/5 - 1s - 144ms/step - loss: 0.1421 - val_loss: 0.1602\n",
      "Epoch 52/75\n",
      "5/5 - 1s - 160ms/step - loss: 0.1322 - val_loss: 0.1594\n",
      "Epoch 53/75\n",
      "5/5 - 1s - 150ms/step - loss: 0.1297 - val_loss: 0.1528\n",
      "Epoch 54/75\n",
      "5/5 - 1s - 151ms/step - loss: 0.1279 - val_loss: 0.1684\n",
      "Epoch 55/75\n",
      "5/5 - 1s - 150ms/step - loss: 0.1236 - val_loss: 0.1451\n",
      "Epoch 56/75\n",
      "5/5 - 1s - 146ms/step - loss: 0.1185 - val_loss: 0.1424\n",
      "Epoch 57/75\n",
      "5/5 - 1s - 143ms/step - loss: 0.1159 - val_loss: 0.1418\n",
      "Epoch 58/75\n",
      "5/5 - 1s - 156ms/step - loss: 0.1106 - val_loss: 0.1517\n",
      "Epoch 59/75\n",
      "5/5 - 1s - 147ms/step - loss: 0.1159 - val_loss: 0.1300\n",
      "Epoch 60/75\n",
      "5/5 - 1s - 152ms/step - loss: 0.1102 - val_loss: 0.1453\n",
      "Epoch 61/75\n",
      "5/5 - 1s - 153ms/step - loss: 0.1050 - val_loss: 0.1352\n",
      "Epoch 62/75\n",
      "5/5 - 1s - 156ms/step - loss: 0.1002 - val_loss: 0.1271\n",
      "Epoch 63/75\n",
      "5/5 - 1s - 170ms/step - loss: 0.1046 - val_loss: 0.1395\n",
      "Epoch 64/75\n",
      "5/5 - 1s - 153ms/step - loss: 0.1000 - val_loss: 0.1299\n",
      "Epoch 65/75\n",
      "5/5 - 1s - 147ms/step - loss: 0.0973 - val_loss: 0.1177\n",
      "Epoch 66/75\n",
      "5/5 - 1s - 153ms/step - loss: 0.0983 - val_loss: 0.1316\n",
      "Epoch 67/75\n",
      "5/5 - 1s - 155ms/step - loss: 0.0962 - val_loss: 0.1172\n",
      "Epoch 68/75\n",
      "5/5 - 1s - 155ms/step - loss: 0.0915 - val_loss: 0.1257\n",
      "Epoch 69/75\n",
      "5/5 - 1s - 154ms/step - loss: 0.0895 - val_loss: 0.1210\n",
      "Epoch 70/75\n",
      "5/5 - 1s - 147ms/step - loss: 0.0896 - val_loss: 0.1181\n",
      "Epoch 71/75\n",
      "5/5 - 1s - 153ms/step - loss: 0.0869 - val_loss: 0.1271\n",
      "Epoch 72/75\n",
      "5/5 - 1s - 149ms/step - loss: 0.0881 - val_loss: 0.1280\n",
      "Epoch 73/75\n",
      "5/5 - 1s - 157ms/step - loss: 0.0842 - val_loss: 0.1222\n",
      "Epoch 74/75\n",
      "5/5 - 1s - 156ms/step - loss: 0.0841 - val_loss: 0.1122\n",
      "Epoch 75/75\n",
      "5/5 - 1s - 150ms/step - loss: 0.0806 - val_loss: 0.1096\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# train the model\n",
    "history = best_model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "# Make predictions\n",
    "predictions_scaled = best_model.predict(X_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "62bd6363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "016ae6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE LSTM: 1.6154865026474\n",
      "RMSE LSTM: 19772.81640625\n",
      "MAE LSTM: 12819.42578125\n"
     ]
    }
   ],
   "source": [
    "# Let's reshape the predictions and Y_val to revert the scaling\n",
    "# Reshape predictions to 2D\n",
    "predictions_scaled_2d = predictions_scaled.reshape(-1, 1)\n",
    "# Get the last timestep of X_val\n",
    "X_val_last_timestep = X_val[:, -1, :]\n",
    "# Replace the first column of X_val_last_timestep with the scaled predictions.\n",
    "X_val_last_timestep[:, 0] = predictions_scaled_2d[:, 0]\n",
    "# unscale the predictions\n",
    "predictions_rescaled = scaler.inverse_transform(X_val_last_timestep)[:, 0]\n",
    "\n",
    "# unscale the Y_val\n",
    "#Y_val_2d = Y_val.reshape(-1, 1)\n",
    "#Y_val_rescaled = scaler.inverse_transform(val.iloc[n_past:, :].values)[:, 0]\n",
    "Y_val_rescaled = scaler.inverse_transform(val.iloc[-len(predictions_scaled):, :].values)[:, 0]\n",
    "\n",
    "# Calculate the error\n",
    "mape = mean_absolute_percentage_error(Y_val_rescaled, predictions_rescaled)\n",
    "rmse = np.sqrt(mean_squared_error(Y_val_rescaled, predictions_rescaled))\n",
    "mae = mean_absolute_error(Y_val_rescaled, predictions_rescaled)\n",
    "\n",
    "print(f'MAPE LSTM: {mape}')\n",
    "print(f'RMSE LSTM: {rmse}')\n",
    "print(f'MAE LSTM: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "32e719f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n"
     ]
    }
   ],
   "source": [
    "# Let's predict the test set using the best model\n",
    "predictions_test_scaled = best_model.predict(X_test)\n",
    "# Make predictions\n",
    "#predictions_test_scaled = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a80f71c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reshape the predictions and Y_val to revert the scaling\n",
    "# Reshape predictions to 2D\n",
    "predictions_test_scaled_2d = predictions_test_scaled.reshape(-1, 1)\n",
    "# Get the last timestep of X_test\n",
    "X_test_last_timestep = X_test[:, -1, :]\n",
    "# Replace the first column of X_test_last_timestep with the scaled predictions.\n",
    "X_test_last_timestep[:, 0] = predictions_test_scaled_2d[:, 0]\n",
    "# unscale the predictions\n",
    "predictions_test_rescaled = scaler.inverse_transform(X_test_last_timestep)[:, 0]\n",
    "\n",
    "# Let's convert the predictions and Y_test to a dataframe usind the index from test\n",
    "predictions_test_df = pd.DataFrame(predictions_test_rescaled, index=test.index[-len(predictions_test_rescaled):], columns=[target_variable])\n",
    "\n",
    "# Reverse the decomposition of the time series\n",
    "#predictions = recompose_time_series(predictions_test_df, decomp_dict)\n",
    "predictions = predictions_test_df.copy()\n",
    "Y_test = df_adjusted[-len(predictions):][target_variable]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f5a61945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE best LSTM: 1.1143759489059448\n",
      "RMSE best LSTM: 43350.359375\n",
      "MAE best LSTM: 33338.4609375\n"
     ]
    }
   ],
   "source": [
    "# Calculate the error\n",
    "mape_best_LSTM = mean_absolute_percentage_error(Y_test, predictions)\n",
    "rmse_best_LSTM = np.sqrt(mean_squared_error(Y_test, predictions))\n",
    "mae_best_LSTM = mean_absolute_error(Y_test, predictions)\n",
    "\n",
    "print(f'MAPE best LSTM: {mape_best_LSTM}')\n",
    "print(f'RMSE best LSTM: {rmse_best_LSTM}')\n",
    "print(f'MAE best LSTM: {mae_best_LSTM}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot LSTM best Model Forecast(static) vs Actuals\n",
    "\n",
    "# Convert Y_test_rescaled to a dataframe using the test index\n",
    "Y_test_rescaled_df = pd.DataFrame(Y_test, index=Y_test.index)\n",
    "#Y_test_rescaled_df = pd.DataFrame(Y_test, index=test.index[n_past:])\n",
    "# Change the column name to the target variable\n",
    "Y_test_rescaled_df.columns = [target_variable]\n",
    "\n",
    "uf.plot_forecast_vs_test(\n",
    "    target_variable, \n",
    "    #train[target_variable], \n",
    "    Y_test_rescaled_df[target_variable],\n",
    "    predictions,\n",
    "    #predictions_test_rescaled, \n",
    "    'LSTM Model Forecast vs Actuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337dee0",
   "metadata": {},
   "source": [
    "Let's forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a419f2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n"
     ]
    }
   ],
   "source": [
    "# Adjust the index to datetime format\n",
    "df_adjusted.index = pd.to_datetime(df_adjusted.index)\n",
    "\n",
    "# New list to store the forecasts\n",
    "monthly_forecasts = []\n",
    "\n",
    "# Define a start date for the forecast\n",
    "start_date = pd.Timestamp(year=2024, month=1, day=1)\n",
    "\n",
    "for i in range(12):  # For each month in the next year\n",
    "    # Define a end date for the forecast\n",
    "    end_date = start_date - pd.DateOffset(months=13)\n",
    "    # Define a start date 12 months before the end date\n",
    "    start_date_12_months_before = end_date - pd.DateOffset(months=11)\n",
    "    # Subset the dataset to get the data for the last 12 months\n",
    "    data_for_prediction = df_adjusted.loc[start_date_12_months_before:end_date]\n",
    "    # Scale the data\n",
    "    data_for_prediction_scaled = scaler.transform(data_for_prediction)\n",
    "    data_for_prediction_scaled_df = pd.DataFrame(data_for_prediction_scaled, columns=df_adjusted.columns) # convert to dataframe\n",
    "    # Include the index in the train and test sets\n",
    "    data_for_prediction_scaled_df.index = data_for_prediction.index\n",
    "    \n",
    "    # Prepare the data for the model\n",
    "    X_predict = np.array([data_for_prediction_scaled_df]) # convert to numpy array\n",
    "    # Predict the next month\n",
    "    future_prediction_scaled = best_model.predict(X_predict)\n",
    "    \n",
    "    # Reverse the scaling\n",
    "    temp_array = np.zeros((1, len(df_adjusted.columns))) # Create an array of zeros\n",
    "    temp_array[0, 0] = future_prediction_scaled[0, 0] # Store the prediction in the first column\n",
    "\n",
    "    # rescale the prediction\n",
    "    future_prediction_rescaled = scaler.inverse_transform(temp_array)[0, 0]\n",
    "\n",
    "    ### Compute confidence intervals\n",
    "    # Compute squared errors for each prediction point\n",
    "    predictions_array = predictions.values.flatten() # Convert the predictions to a 1D array\n",
    "    squared_errors = (Y_test - predictions_array) ** 2\n",
    "    rmse_errors = np.sqrt(squared_errors) # Take the square root of each squared error to get the RMSE for each point\n",
    "    # Compute the standard error\n",
    "    error_std = np.std(rmse_errors, axis=0)\n",
    "    # Apply it to your forecasts\n",
    "    lower_bound = future_prediction_rescaled - 1.96 * error_std # 95% confidence interval\n",
    "    upper_bound = future_prediction_rescaled + 1.96 * error_std\n",
    "\n",
    "\n",
    "    # Append the prediction to the list\n",
    "    #monthly_forecasts.append(future_prediction_rescaled)\n",
    "    # Store the prediction and bounds\n",
    "    monthly_forecasts.append((future_prediction_rescaled, lower_bound, upper_bound))\n",
    "    # Change the start date to the next month\n",
    "    start_date += pd.DateOffset(months=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a8bde",
   "metadata": {},
   "source": [
    "### Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99e6e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the forecasts to a DataFrame\n",
    "# Get the last date of the test set\n",
    "last_test_date = Y_test_rescaled_df.index[-1]\n",
    "future_dates = pd.date_range(start=last_test_date + pd.DateOffset(months=1), periods=12, freq='MS')\n",
    "monthly_forecasts_df = pd.DataFrame(monthly_forecasts, index=future_dates, columns=[target_variable, 'Lower Bound', 'Upper Bound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d9413d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the monthly forecasts to a DataFrame\n",
    "#monthly_forecasts = pd.DataFrame(monthly_forecasts, index=future_dates, columns=[target_variable])\n",
    "\n",
    "# Convert future_series to a DataFrame\n",
    "#future_series_df = pd.DataFrame(future_series, index=future_dates, columns=[target_variable])\n",
    "\n",
    "# Plot the actuals, test predictions, and future forecasts\n",
    "plt.figure(figsize=(14, 7))\n",
    "# Actuals for the test set\n",
    "#plt.plot(Y_test_rescaled_df.index, Y_test_rescaled_df[target_variable], label='Actuals', marker='o')\n",
    "# Actuals for the test set\n",
    "plt.plot(Y_test_rescaled_df.index, Y_test_rescaled_df[target_variable], label='Actuals', marker='o', color='blue')\n",
    "# Predictions for the test set\n",
    "plt.plot(predictions.index, predictions[target_variable], label='Test Predictions', marker='x', color='orange')\n",
    "\n",
    "# Future forecasts with confidence intervals\n",
    "plt.plot(monthly_forecasts_df.index, monthly_forecasts_df[target_variable], label='Future Forecasts', marker='*', color='red')\n",
    "\n",
    "# Confidence intervals\n",
    "plt.fill_between(monthly_forecasts_df.index, \n",
    "                 monthly_forecasts_df['Lower Bound'], \n",
    "                 monthly_forecasts_df['Upper Bound'], \n",
    "                 color='red', alpha=0.2, label='95% Confidence Interval')\n",
    "\n",
    "# Predictions for the test set\n",
    "#plt.plot(predictions.index, predictions[target_variable], label='Test Predictions', marker='x')\n",
    "# Future forecasts decomposing the original series from the last 12 months and them recomposing after the forecast\n",
    "# plt.plot(future_series_df.index, future_series_df[target_variable], label='Future Forecasts - Decomposing', marker='*')\n",
    "# Future forecasts based on the complete original series from the last 12 months\n",
    "#plt.plot(future_series_df.index, monthly_forecasts, label='Future Forecasts - Complete Series', marker='*')\n",
    "\n",
    "# Annotate the last predicted observation\n",
    "last_predicted_date = predictions.index[-1]\n",
    "last_predicted_value = predictions[target_variable].iloc[-1]\n",
    "plt.scatter(last_predicted_date, last_predicted_value, color='orange')  # Highlight the last point\n",
    "plt.text(last_predicted_date, last_predicted_value, f' R${last_predicted_value:,.2f}', color='orange', va='bottom', ha='right')\n",
    "\n",
    "# # Annotate the last forecast observation - decomposing\n",
    "# last_forecast_date = future_series_df.index[-1]\n",
    "# last_forecast_value = future_series_df[target_variable].iloc[-1]\n",
    "# plt.scatter(last_forecast_date, last_forecast_value, color='green')  # Highlight the last point\n",
    "# plt.text(last_forecast_date, last_forecast_value, f' R${last_forecast_value:,.2f}', color='green', va='bottom', ha='right')\n",
    "\n",
    "# Annotate the last forecast observation - Complete Series\n",
    "last_forecast_date = monthly_forecasts_df.index[-1]\n",
    "last_forecast_value = monthly_forecasts_df[target_variable].iloc[-1]\n",
    "plt.scatter(last_forecast_date, last_forecast_value, color='red')  # Highlight the last point\n",
    "plt.text(last_forecast_date, last_forecast_value, f' R${last_forecast_value:,.2f}', color='red', va='center', ha='left')\n",
    "\n",
    "plt.title('LSTM Model Forecast (Static) vs Actuals with Confidence Intervals')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel(target_variable)\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
